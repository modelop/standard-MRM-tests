{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Risk Test Developer\n",
    "\n",
    "## Provide ModelOp Center and S3 Configuration\n",
    "\n",
    "In order to read information from ModelOp Center, we must provide the location of where ModelOp Center is installed and\n",
    "our S3 access credentials.  Please run the cell and fill them below and click the Update button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0670577e6d4c7ba1054517ccedd3aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='ModelOp Center URL')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eba684de862b4469a357c8b8a8873e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='https://mocaasin.modelop.center/')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "064c43537c874d248e85bad69f950997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='S3 Access Key')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28d1dc4962b943a5865e9b5cf53330e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Password()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9303b4318e424d57a2de827d95667880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='S3 Secret Key')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a1ab6b6f39442c9b59406d748393e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Password()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "411d8b13caa8483e952528a4a7871da9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Cognito App Client Id')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01ff7b3d61ed4c588e5616fb58ce9205",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Password()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02b4581b24542e3b9e6e9cbdbc4fea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Cognito App Client Secret')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62235fa9f82146fa8106e8664e46c155",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Password()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a29389acac4f43a9a1be37e447d2ac5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Cognito User Pool Id')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e68aed777800480aacde120ac343f79d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Password()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b52c21db86444f4bbe398dc60cf66fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Cognito Identity Pool Id')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4f6c0dc9b9f4f7693ce3b85af573096",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Password()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04bf6f5afc4348eebe9d86a14401db5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Label(value='Role Arn')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ed97129a44343aa8a473ae0fb2c7d9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Password()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4536ca33c9d2484fba624813bde7f197",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Update Values', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c71587e7d44f4f8531b4efd65586e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid black', border_left='1px solid black', border_right='1px solid b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import IFrame, display, HTML\n",
    "import os\n",
    "import requests\n",
    "from ipywidgets import widgets, HBox, VBox, Label\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import json\n",
    "import ipyfilechooser\n",
    "from ipyfilechooser import FileChooser\n",
    "\n",
    "nest_asyncio.apply()\n",
    "moc_url = \"http://localhost:8090/\"\n",
    "# moc_url = \"https://mocaasin.modelop.center/\"\n",
    "os.environ[\"modelop.gateway-url\"] = moc_url\n",
    "config_out = widgets.Output(layout={'border': '1px solid black'})\n",
    "s3_access_key = \"AKIAIOSFODNN7EXAMPLE\"\n",
    "s3_secret_key = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n",
    "client_id = os.getenv(\"COGNITO_CLIENT_ID\", \"123exampleclientid2342423\")\n",
    "client_secret = os.getenv(\"COGNTIO_CLIENT_SECRET\", \"141231exampleclientsecert13214\")\n",
    "cognito_identity_pool_id = os.getenv(\"COGNITO_IDENTITY_POOL_ID\", \"us-region-5:dafsab-asds-sdfs-dfss-someidentpoolid\")\n",
    "cognito_user_pool_id = os.getenv(\"COGNITO_USER_POOL_ID\",\"cognito-idp.us-region-5.amazonaws.com/us-region-5_someusergroupid\")\n",
    "role_arn = \"arn:aws:iam::someaccountnum:role/an-example-role\"\n",
    "\n",
    "def update_values(button):\n",
    "\tglobal moc_url, s3_access_key, s3_secret_key, client_id, client_secret, cognito_identity_pool_id, cognito_user_pool_id, role_arn\n",
    "\tmoc_url = moc_url_box.value\n",
    "\ts3_access_key = s3_access_key_box.value\n",
    "\ts3_secret_key = s3_secret_key_box.value\n",
    "\tclient_id = client_id_box.value\n",
    "\tclient_secret = client_secret_box.value\n",
    "\tcognito_user_pool_id = cognito_user_pool_box.value\n",
    "\tcognito_identity_pool_id = cognito_identity_pool_box.value\n",
    "\trole_arn = role_arn_box.value\n",
    "\twith config_out:\n",
    "\t\tprint(\"Using modelop center url: \" + moc_url)\n",
    "\t\tprint(\"S3 Keys Updated\")\n",
    "\n",
    "display(widgets.Label('ModelOp Center URL'))\n",
    "moc_url_box = widgets.Text(value=moc_url)\n",
    "display(moc_url_box)\n",
    "display(widgets.Label('S3 Access Key'))\n",
    "s3_access_key_box = widgets.Password(value=s3_access_key)\n",
    "display(s3_access_key_box)\n",
    "display(widgets.Label('S3 Secret Key'))\n",
    "s3_secret_key_box = widgets.Password(value=s3_secret_key)\n",
    "display(s3_secret_key_box)\n",
    "display(widgets.Label('Cognito App Client Id'))\n",
    "client_id_box = widgets.Password(value=client_id)\n",
    "display(client_id_box)\n",
    "display(widgets.Label('Cognito App Client Secret'))\n",
    "client_secret_box = widgets.Password(value=client_secret)\n",
    "display(client_secret_box)\n",
    "display(widgets.Label('Cognito User Pool Id'))\n",
    "cognito_user_pool_box = widgets.Password(value=cognito_user_pool_id)\n",
    "display(cognito_user_pool_box)\n",
    "display(widgets.Label('Cognito Identity Pool Id'))\n",
    "cognito_identity_pool_box = widgets.Password(value=cognito_identity_pool_id)\n",
    "display(cognito_identity_pool_box)\n",
    "display(widgets.Label('Role Arn'))\n",
    "role_arn_box = widgets.Password(value=role_arn)\n",
    "display(role_arn_box)\n",
    "update_button = widgets.Button(description='Update Values')\n",
    "update_button.on_click(update_values)\n",
    "display(update_button)\n",
    "display(config_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "require.undef('sign_in_widget');\n",
    "\n",
    "function getJupyterUrl () {\n",
    "    return location.protocol+'//' + location.hostname + (location.port ? ':'+location.port: '');\n",
    "}\n",
    "\n",
    "function removeEndingSlash( url ) {\n",
    "    return url.replace(/\\/$/, \"\");\n",
    "}\n",
    "\n",
    "function randomString(length) {\n",
    "    var chars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890,./;'[]\\=-)(*&^%$#@!~`\";\n",
    "    var result = \"\";\n",
    "    for (var i = length; i > 0; --i) result += chars[Math.floor(Math.random() * chars.length)];\n",
    "    return result;\n",
    "}\n",
    "\n",
    "define('sign_in_widget', [\"@jupyter-widgets/base\"], function(widgets) {\n",
    "\n",
    "    var SignInView = widgets.DOMWidgetView.extend({\n",
    "\n",
    "        getOAuth2Token: function( widget, customPagePath, finalBaseUrl ) {\n",
    "            finalBaseUrl = removeEndingSlash(finalBaseUrl)\n",
    "            $.ajax({\n",
    "                type: 'GET',\n",
    "                url: finalBaseUrl + '/api/oauth2/.well-known-configuration/jupyter',\n",
    "                success: function( response ) {\n",
    "                    widget.redirectToLogin(widget, customPagePath, finalBaseUrl, response);\n",
    "                },\n",
    "                error: function( jqXHR, textStatus, error ) {\n",
    "                    if (jqXHR.status == 404) {\n",
    "                        widget.login_result.textContent = \"Login not required (or no well known configuration found for jupyter)\";\n",
    "                        widget.model.save_changes();\n",
    "                    } else {\n",
    "                        console.log(\"An error occurred while trying to get the authorization uri from gateway service.\");\n",
    "                        widget.login_result.textContent = \"Couldn't connect with the provided Url: \" + finalBaseUrl;\n",
    "                    }\n",
    "                }\n",
    "            });\n",
    "        },\n",
    "\n",
    "        redirectToLogin: function (widget, customPagePath, finalBaseUrl, oAuth2Data ) {\n",
    "            let formattedScope = oAuth2Data.scopes.join(\" \");\n",
    "\n",
    "            var redirectUriParams = {\n",
    "                client_id: oAuth2Data.clientId,\n",
    "                response_type: oAuth2Data.responseType,\n",
    "                scope: formattedScope,\n",
    "                state: getJupyterUrl() + customPagePath,\n",
    "                nonce: randomString(10),\n",
    "                redirect_uri: oAuth2Data.redirectUri\n",
    "            };\n",
    "            widget.displayClientAuthorization(widget, oAuth2Data.oAuth2Provider.authorizationUri + \"?\" + $.param(redirectUriParams));\n",
    "        },\n",
    "\n",
    "        displayClientAuthorization: function (widget, url ) {\n",
    "            var w = 500;\n",
    "            var h = 600;\n",
    "            const y = window.top.outerHeight / 2 + window.top.screenY - ( h / 2);\n",
    "            const x = window.top.outerWidth / 2 + window.top.screenX - ( w / 2);\n",
    "            var login_popup = window.open(url, 'Client Authorization', `toolbar=no, location=no, directories=no, status=no, menubar=no, scrollbars=no, resizable=no, copyhistory=no, width=${w}, height=${h}, top=${y}, left=${x}`);\n",
    "            var timer = setInterval(function() {\n",
    "                try {\n",
    "                    if(login_popup.closed) {\n",
    "                        clearInterval(timer);\n",
    "                        if (!widget.token_input.value) {\n",
    "                            widget.login_result.textContent = \"Login interrupted!\"\n",
    "                        }\n",
    "                    }\n",
    "                    try {\n",
    "                        if (login_popup.location?.hash) {\n",
    "                            let parsedHash = new URLSearchParams(login_popup.location.hash.substring(1));\n",
    "                            widget.model.set('token', parsedHash.get(\"access_token\"));\n",
    "                            widget.login_result.textContent = \"Login successful!\"\n",
    "                            widget.model.save_changes();\n",
    "                            login_popup.close()\n",
    "                        }\n",
    "                    } catch {}\n",
    "                } catch {\n",
    "                    clearInterval(timer);\n",
    "                }\n",
    "            }, 200);\n",
    "        },\n",
    "    \n",
    "        // Render the view.\n",
    "        render: function() {\n",
    "            this.getOAuth2Token(this, \"/\", this.model.get('base_url'))\n",
    "            this.login_result = document.createElement('div');\n",
    "            this.token_input = document.createElement('hidden');\n",
    "            this.token_input.type = 'text';\n",
    "            this.token_input.value = this.model.get('token');\n",
    "            this.token_input.disabled = this.model.get('disabled');\n",
    "\n",
    "            this.el.appendChild(this.login_result);\n",
    "            this.el.appendChild(this.token_input);\n",
    "\n",
    "            // Python -> JavaScript update\n",
    "            this.model.on('change:token', this.value_changed, this);\n",
    "            this.model.on('change:disabled', this.disabled_changed, this);\n",
    "\n",
    "            // JavaScript -> Python update\n",
    "            this.token_input.onchange = this.input_changed.bind(this);\n",
    "        },\n",
    "\n",
    "        value_changed: function() {\n",
    "            this.token_input.value = this.model.get('token');\n",
    "        },\n",
    "\n",
    "        disabled_changed: function() {\n",
    "            this.token_input.disabled = this.model.get('disabled');\n",
    "        },\n",
    "\n",
    "        input_changed: function() {\n",
    "            this.model.set('token', this.token_input.value);\n",
    "            this.model.save_changes();\n",
    "        },\n",
    "    });\n",
    "\n",
    "    return {\n",
    "        SignInView: SignInView\n",
    "    };\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31312206779456db697eec34b461640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "SignInWidget(base_url='https://mocaasin.modelop.center/')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from traitlets import Unicode, Bool\n",
    "from ipywidgets import DOMWidget, register\n",
    "\n",
    "\n",
    "@register\n",
    "class SignInWidget(DOMWidget):\n",
    "    _view_name = Unicode('SignInView').tag(sync=True)\n",
    "    _view_module = Unicode('sign_in_widget').tag(sync=True)\n",
    "    _view_module_version = Unicode('0.1.0').tag(sync=True)\n",
    "\n",
    "    # Attributes\n",
    "    token = Unicode(\"\", help=\"The oauth2 token obtained.\").tag(sync=True)\n",
    "    disabled = Bool(True, help=\"Enable or disable user changes.\").tag(sync=True)\n",
    "    base_url = Unicode(\"http://localhost:8090\", help=\"The base URL\").tag(sync=True)\n",
    "\n",
    "\n",
    "login = SignInWidget(base_url = moc_url)\n",
    "display(login)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Target Model\n",
    "\n",
    "A dashboard or standardized test model runs against a given target model.  It will pull the required data resources,\n",
    "such as baseline and/or comparator data, from this target model.  So in developing your dashboard or standardized test,\n",
    "we will start with picking an example target model that is currently registered with ModelOp Center to run our dashboard\n",
    "or test against.  This model should already have baseline and comparator data defined as assets stored in S3.  Run the\n",
    "cell and select one of the models in the drop down box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df06b96669404aaa84b077b846924640",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Dropdown(description='Target Model:', options=(('add10941', '574938e2-71d4-4cc1-b998-6ce39c292bc0'), ('add1414…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "session = requests.Session()\n",
    "session.headers.update({'Authorization': f'Bearer {login.token}'})\n",
    "try:\n",
    "\tresponse = session.get(moc_url + 'model-manage/api/storedModelSummaries?size=500')\n",
    "\n",
    "\tavailable_models = []\n",
    "\tresponse.raise_for_status()\n",
    "\tif response.ok:\n",
    "\t\tmodel_list = response.json()\n",
    "\t\tfor model in model_list[\"_embedded\"][\"storedModelSummaries\"]:\n",
    "\t\t\tavailable_models.append((model[\"modelMetaData\"][\"name\"], model[\"id\"]))\n",
    "\tavailable_models = sorted(available_models, key=lambda v: v[0].upper())\n",
    "\tmodel_selector = widgets.Dropdown(options=available_models, description='Target Model:')\n",
    "\tdisplay(model_selector)\n",
    "except Exception as e:\n",
    "\tdisplay(HTML('Couldn\\'t connect to ' + moc_url + '</br><pre><code>' + str(e) + '</code></pre>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Target Model\n",
    "\n",
    "By running the cell below, we will contact model manage and fetch the definition of the example target model.\n",
    "This information will be used for running your Standard Risk Test against an existing model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"MODELOP_GATEWAY_LOCATION\"]=moc_url\n",
    "os.environ[\"MODELOP_OAUTH_ACCESS_TOKEN\"]=login.token\n",
    "\n",
    "model = {}\n",
    "response = session.get(moc_url + 'model-manage/api/storedModels/' + model_selector.value)\n",
    "if response.ok:\n",
    "\tmodel = response.json()\n",
    "\tdisplay(VBox([\n",
    "\t\tHBox([Label(value=\"Model Name:\"), Label(model.get(\"modelMetaData\", {}).get(\"name\", \"Name not set\"))]),\n",
    "\t\tHBox([Label(value=\"Description:\"), Label(model.get(\"modelMetaData\", \"\").get(\"description\", \"\"))]),\n",
    "\t\tHBox([Label(value=\"Created By:\"), Label(model.get(\"createdBy\", \"Unknown\"))]),\n",
    "\t\tHBox([Label(value=\"Created Date:\"), Label(model.get(\"createdDate\", \"Unknown\"))])\n",
    "\t]))\n",
    "\tdisplay(HTML(\"Model loaded and can be viewed <a href=\" + moc_url + \"#/models/business-models/\" + model[\"id\"] + \" target=\\\"_blank\\\" >here</a>.\"))\n",
    "else:\n",
    "\tprint(\"ERROR - The model could not be found, please update the example_model_name variable with the correct model name to examine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Check for valid schema\n",
    "\n",
    "Let's now check that the target model has the schema populated.  All target models for dashboards and standardized tests\n",
    "will typically require an input schema definition.  This allows us to know what columns are used for labels, inputs, and\n",
    "which columns are a protected class, as well as other pertinent information about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "column_labels = []\n",
    "input_schema = model.get(\"modelMetaData\", {}).get(\"inputSchema\", [])\n",
    "if len(input_schema) != 1:\n",
    "\tdisplay(HTML(\"The standard risk test model and dashboard model requires exactly one input schema to be specified. Please go to the model's schema page in MOC, and generate and edit a valid schema for the input data or generate the schema in the next cells of this notebook\"))\n",
    "else:\n",
    "\tinput_schema = model.get(\"modelMetaData\", {}).get(\"inputSchema\", [])[0].get(\"schemaDefinition\", {})\n",
    "\tresponse = session.post(moc_url + \"model-manage/api/schema/validate/?extended=true\", json = input_schema)\n",
    "\tif response.ok:\n",
    "\t\tdisplay(HTML(\"The example model contains a valid schema\"))\n",
    "\t\tfor col in input_schema.get(\"fields\", []):\n",
    "\t\t\tcolumn_labels.append(col.get(\"name\", \"\"))\n",
    "\t\tdisplay(column_labels)\n",
    "\telse:\n",
    "\t\tdisplay(HTML(\"The standard risk test model and dashboard model requires a valid extended schema definition. Please go to the model's schema page in MOC, and generate and edit a valid schema for the input data or validate and check schema in the next cells of this notebook\"))\n",
    "display(HTML(\"You can view and edit the schema in ModelOp Center <a target=\\\"_blank\\\" href=\\\"\" + moc_url + \"#/models/business-models/\" + model[\"id\"] + \"/schema\\\">here</a>.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Schema [Skip if using Model's current schema ](#load_schema)\n",
    "Here you are able to generate schema from a json or csv file on your local device.\n",
    "\n",
    "if you wish to skip this schema generation and continue with the notebook and validate your current schema please skip this section and go to the [Load Schema](#load_schema) Cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting sample data file to generate schema\n",
    "Please select the sample data file you want to use to generate your schema off of and continue to the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_path= os.path.abspath(\"standard_risk_test_designer.ipynb\")\n",
    "fc = FileChooser(os.path.dirname(notebook_path))\n",
    "display(fc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Schema from sample data file\n",
    "Now we send the data to the endpoint to generate a schema file, please select the generate schema button to generate the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modelop_sdk.restclient.moc_client as moc_client\n",
    "import modelop_sdk.apis.model_manage_api as mm\n",
    "\n",
    "\n",
    "with open(os.path.join(fc.selected_path, fc.selected_filename)) as f:\n",
    "    client = moc_client.MOCClient()\n",
    "    mm_api = mm.ModelManageApi(client)\n",
    "    input_schema = mm_api.generate_schema(data=f, extended_schema=True)\n",
    "    print(json.dumps(input_schema, indent=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='load_schema'></a>\n",
    "## Load Schema \n",
    "\n",
    "By running the cell below you will see a cell created with your model repository's schema, here you can make any changes you wish, or leave the schema unchanged. Each time this cell is run it will create a new cell with the model repository's schema so if running multiple times remember to remove old schema cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "\n",
    "function loadInJs() {\n",
    "    Jupyter.notebook.kernel.execute(\"import json\")\n",
    "    Jupyter.notebook.kernel.execute(\n",
    "    \"input_schema\", { iopub: { output: callBack.bind(this) } }, { silent: false }\n",
    "    );\n",
    "}\n",
    "    \n",
    "function callBack(msg) {\n",
    "    let codeCell = Jupyter.notebook.insert_cell_below('code', cell_idx);\n",
    "    codeCell.set_text((msg.content.data['text/plain']));\n",
    "}\n",
    "\n",
    "let output_area = this;\n",
    "let cell_element = output_area.element.parents('.cell');\n",
    "let cell_idx = Jupyter.notebook.get_cell_elements().index(cell_element);\n",
    "loadInJs();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Updated Schema\n",
    "\n",
    "Here we are saving what edits you have made to your schema in the previous cell above in order to check that it fulfills the requirements for our metrics. This works by reading the cell directly before this text cell and saving it to the specified value below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%js\n",
    "\n",
    "let output_area = this;\n",
    "let cell_element = output_area.element.parents('.cell');\n",
    "let cell_idx = Jupyter.notebook.get_cell_elements().index(cell_element);\n",
    "\n",
    "function reloadVar(msg) {\n",
    "    element.append(\"Error processing json as python: \"+ msg.content.evalue);\n",
    "}\n",
    "\n",
    "Jupyter.notebook.kernel.execute\n",
    "(\"input_schema=\" +\n",
    "     Jupyter.notebook.get_cell(cell_idx - 2).get_text()\n",
    "    , { iopub: { output: reloadVar.bind(this) } }, { silent: false }\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Updated Schema for All Metrics\n",
    "\n",
    "Below we are validating the updated schema from the cell above. Here we will check the basic requirements for all of our metrics and warn you if you are missing values or formatted anything incorrectly. If you see a warn message for metrics you are not running, you may ignore it and continue on without issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics={\"classification performance\", \"regression performance\", \"bias\", \"drift\", \"concept drift\", \"volumetrics\",\"stability\", \"ROI\", \"Default Dashboard\", \"autocorrelation\", \"homoscedasticity\", \"normality\", \"multicollinearity\", \"linearity\"}\n",
    "\n",
    "#Checking for dataClass and role, which is essential for all metrics\n",
    "for col in input_schema['fields']:\n",
    "    if not col.get(\"dataClass\") in {\"numerical\", \"categorical\"}:\n",
    "        print('\\nWARN: dataClass attribute is required in', col.get(\"name\", \"\"), 'field for all metrics.')\n",
    "        metrics={0}\n",
    "\n",
    "    if not col.get(\"role\") in {\"label\", \"score\", \"predictor\", \"non-predictor\", \"weight\", \"identifier\"}:  \n",
    "        print('\\nWARN: role attribute is required in', col.get(\"name\", \"\"), 'field for all metrics ')\n",
    "        metrics={0}\n",
    "\n",
    "        \n",
    "#Checking for classification metric\n",
    "value_indicator=0\n",
    "for col in input_schema['fields']:\n",
    "    if (col.get(\"role\")== \"label\" and col.get(\"dataClass\")== \"categorical\"):\n",
    "        value_indicator=1\n",
    "        break\n",
    "if value_indicator!=1:\n",
    "    metrics.discard(\"classification performance\")\n",
    "    print(\"\\nWARN: You must have 1 column with role=label (ground truth) and dataClass=categorical to run the performance monitor Classification\")        \n",
    "        \n",
    "value_indicator=0\n",
    "for col in input_schema['fields']:\n",
    "    if col.get(\"role\")== \"score\" and col.get(\"dataClass\")== \"categorical\":\n",
    "        value_indicator=1\n",
    "        break\n",
    "if value_indicator!=1:\n",
    "    metrics.discard(\"classification performance\")\n",
    "    print(\"\\nWARN: You must have 1 column with role=score (model output) and dataClass=categorical to run the performance monitor Classification\")        \n",
    "                \n",
    "        \n",
    "        \n",
    "#Checking for regression metric\n",
    "value_indicator=0\n",
    "for col in input_schema['fields']:\n",
    "    if col.get(\"role\")== \"label\" and col.get(\"dataClass\")== \"numerical\":\n",
    "        value_indicator=1\n",
    "        break\n",
    "if value_indicator!=1:\n",
    "    metrics.discard(\"regression performance\")\n",
    "    print(\"\\nWARN: You must have 1 column with role=label (ground truth) and dataClass=numerical to run the performance monitor Regression\")        \n",
    "        \n",
    "value_indicator=0\n",
    "for col in input_schema['fields']:\n",
    "    if col.get(\"role\")== \"score\" and col.get(\"dataClass\")== \"numerical\":\n",
    "        value_indicator=1\n",
    "        break\n",
    "if value_indicator!=1:\n",
    "    metrics.discard(\"regression performance\")\n",
    "    print(\"\\nWARN: You must have 1 column with role=score (model output) and dataClass=numerical to run the performance monitor Regression\")        \n",
    "\n",
    "    \n",
    "#Checking for PREDICTOR role for relevant metrics\n",
    "value_indicator=0\n",
    "for col in input_schema['fields']:\n",
    "    if col.get(\"role\")== \"predictor\":\n",
    "        value_indicator=1\n",
    "        break\n",
    "if value_indicator!=1:\n",
    "    metrics.discard(\"stability\")\n",
    "    metrics.discard(\"drift\")\n",
    "    metrics.discard(\"Default Dashboard\")\n",
    "    print(\"\\nWARN: You must have 1 column with role=predictor to run the Drift metrics, Stability metrics and Default Dashboard\")        \n",
    "    \n",
    "#Checking for SCORE role for relevant metrics\n",
    "value_indicator=0\n",
    "for col in input_schema['fields']:\n",
    "    if col.get(\"role\")== \"score\":\n",
    "        value_indicator=1\n",
    "        break\n",
    "if value_indicator!=1:\n",
    "    metrics.discard(\"normality\")\n",
    "    metrics.discard(\"homoscedasticity\")\n",
    "    metrics.discard(\"autocorrelation\")\n",
    "    metrics.discard(\"stability\")\n",
    "    metrics.discard(\"concept drift\")\n",
    "    metrics.discard(\"bias\")\n",
    "    metrics.discard(\"ROI\")\n",
    "    metrics.discard(\"Default Dashboard\")\n",
    "    print(\"\\nWARN: You must have 1 column with role=score to run the Concept Drift metrics, Bias metrics, Stability metrics, ROI[Actual, Projected], Autocorrelation, Homoscedasticity, Normality, and Default Dashboard\")      \n",
    "\n",
    "    \n",
    "#Checking for LABEL role for relevant metrics\n",
    "value_indicator=0\n",
    "for col in input_schema['fields']:\n",
    "    if col.get(\"role\")== \"label\":\n",
    "        value_indicator=1\n",
    "        break\n",
    "if value_indicator!=1:\n",
    "    metrics.discard(\"normality\")\n",
    "    metrics.discard(\"linearity\")\n",
    "    metrics.discard(\"homoscedasticity\")\n",
    "    metrics.discard(\"autocorrelation\")\n",
    "    metrics.discard(\"bias\")\n",
    "    metrics.discard(\"ROI\")\n",
    "    metrics.discard(\"Default Dashboard\")\n",
    "    print(\"\\nWARN: You must have 1 column with role=label to run the Bias metrics, ROI[Actual, Projected], Autocorrelation, Homoscedasticity, Linearity, Normality, and Default Dashboard\")      \n",
    "    \n",
    "#Checking for IDENTIFIER role for relevant metrics\n",
    "value_indicator=0\n",
    "for col in input_schema['fields']:\n",
    "    if col.get(\"role\")== \"identifier\":\n",
    "        value_indicator=1\n",
    "        break\n",
    "if value_indicator!=1:\n",
    "    metrics.discard(\"volumetrics\")\n",
    "    metrics.add(\"volumetrics[count, count comparison, summary]\")\n",
    "    metrics.discard(\"Default Dashboard\")\n",
    "    print(\"\\nWARN: You must have 1 column with role=identifier to run the Volumetrics[Comparison, Identifier Comparison] metrics and Default Dashboard\")  \n",
    "    \n",
    "#Checking for NUMERICAL dataclass for relevant metrics\n",
    "value_indicator=0\n",
    "for col in input_schema['fields']:\n",
    "    if col.get(\"dataClass\")== \"numerical\":\n",
    "        value_indicator=1\n",
    "        break\n",
    "if value_indicator!=1:\n",
    "    metrics.discard(\"linearity\")\n",
    "    metrics.discard(\"homoscedasticity\")\n",
    "    metrics.discard(\"multicollinearity\")\n",
    "    print(\"\\nWARN: You must have 1 column with dataclass=numerical to run the Homoscedasticity, Multicollinearity, Linearity metrics\")      \n",
    "      \n",
    "#Checking driftCandidate\n",
    "value_indicator=0\n",
    "for col in input_schema['fields']:\n",
    "    if col.get(\"driftCandidate\") == True or col.get(\"driftCandidate\")==1: \n",
    "          value_indicator=1\n",
    "    elif col.get(\"driftCandidate\") == False or col.get(\"driftCandidate\")==0:\n",
    "        continue\n",
    "    else:\n",
    "        metrics.discard(\"drift\")\n",
    "        metrics.discard(\"concept drift\")\n",
    "        metrics.discard(\"Default Dashboard\")\n",
    "        print('\\nWARN: driftCandidate attribute is required as a boolean or [0,1] for Concept Drift, Drift, and Default Dashboard metrics in', col.get(\"name\", \"\"), 'field.')\n",
    "        \n",
    "if value_indicator!=1:\n",
    "    metrics.discard(\"drift\")\n",
    "    metrics.discard(\"concept drift\")\n",
    "    metrics.discard(\"Default Dashboard\")\n",
    "    print(\"\\nWARN: There is not a driftCandidate key with the value True or 1 in your schema, a True value is required for: Concept Drift, Drift, and Default Dashboard metrics.\")\n",
    "        \n",
    "\n",
    "#Checking protectedClass\n",
    "value_indicator=0\n",
    "for col in input_schema['fields']:\n",
    "    if col.get(\"protectedClass\") == True or col.get(\"protectedClass\")==1: \n",
    "          value_indicator=1\n",
    "    elif col.get(\"protectedClass\") == False or col.get(\"protectedClass\")==0:\n",
    "        continue\n",
    "    else:\n",
    "        metrics.discard(\"bias\")\n",
    "        print('\\nWARN: protectedClass attribute is required as a boolean or [0,1] for Bias[Group, Disparity, Comprehensive] metric in', col.get(\"name\", \"\"), 'field.')\n",
    "if value_indicator!=1:\n",
    "    metrics.discard(\"bias\")\n",
    "    print(\"\\nWARN: There is not a protectedClass key with the value True or 1 in your schema, a True value is required for the Bias metric.\")\n",
    "    \n",
    "    \n",
    "#Checking positiveClassLabel\n",
    "value_indicator=0\n",
    "for col in input_schema['fields']:\n",
    "    if col.get(\"positiveClassLabel\") ==1: \n",
    "        value_indicator=1\n",
    "        break\n",
    "if value_indicator!=1:\n",
    "    metrics.discard(\"ROI\")\n",
    "    metrics.discard(\"Default Dashboard\")\n",
    "    print(\"\\nWARN: There is not a positiveClassLabel key with the value 1 in your schema, a value is required for the ROI- Binary Classifiers[Actual, Projected] and the Default Dashboard metrics.\")\n",
    "        \n",
    "        \n",
    "#Checking ROI isAmountField\n",
    "value_indicator=0\n",
    "for col in input_schema['fields']:\n",
    "    if col.get(\"isAmountField\") == True or col.get(\"isAmountField\")==1: \n",
    "        value_indicator=1\n",
    "        break     \n",
    "if value_indicator!=1:\n",
    "    metrics.discard(\"ROI\")\n",
    "    metrics.discard(\"Default Dashboard\")\n",
    "    print(\"\\nWARN: There is not a isAmountField key with the value True or 1 in your schema, a True value is required for the ROI- Binary Classifiers[Actual, Projected] and the Default Dashboard metrics.\")\n",
    "            \n",
    "              \n",
    "        \n",
    "print(\"\\nSchema will run:\",metrics,\" metrics\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving and Updating Schema\n",
    "\n",
    "By running the cell below, you will have the option to save the updated schema you have worked on in the above cells to your device or update the ModelOp Center with it directly, or you can do both. Now it is recommened that you save the file and add this to your model’s git repository and sync the ModelOp UI with the updated repository. You can sync by going to the repository tab in your model on ModelOp UI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import modelop_sdk.restclient.moc_client as moc_client\n",
    "import modelop_sdk.apis.model_manage_api as mm\n",
    "\n",
    "schema_config_out = widgets.Output(layout={'border': '1px solid black'})\n",
    "\n",
    "def download_schema(button):\n",
    "\tglobal input_schema\n",
    "\twith open('input_schema.avsc', 'w') as json_file:\n",
    "\t\tjson.dump(input_schema, json_file)\n",
    "\twith schema_config_out:\n",
    "\t\t print(\"File is saved in\", os.getcwd(), \"\\nPlease add this to your model’s git repository and sync the ModelOp UI with the updated repository. \\nYou can sync by going to the repository tab in your model on ModelOp UI.\")\n",
    "\n",
    "def update_schema(button):\n",
    "\tglobal schema\n",
    "\tclient = moc_client.MOCClient()\n",
    "\tmm_api = mm.ModelManageApi(client)\n",
    "\tschema = mm_api.save_schema_in_model(stored_model_id=model.get(\"id\", {}), schema=input_schema, name=\"input_schema.avsc\", schema_id=\"78aa7aaa-5a0a-46a0-a843-7aa654315375\")\n",
    "\twith schema_config_out:\n",
    "\t\tprint(json.dumps(schema[\"modelMetaData\"][\"inputSchema\"], indent=1))\n",
    "              \n",
    "download_schema_button = widgets.Button(description='Download Schema')\n",
    "download_schema_button.on_click(download_schema)\n",
    "display(download_schema_button)\n",
    "print(\"or\")\n",
    "update_schema_button = widgets.Button(description='Save in ModelOp')\n",
    "update_schema_button.on_click(update_schema)\n",
    "display(update_schema_button)\n",
    "display(schema_config_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-Load Target Model\n",
    "\n",
    "By running the cell below, we will contact model manage and fetch the definition of the example target model after you have updated your model's schema via Git or the UI. If you have not changed your model's schema you may run or skip this cell.\n",
    "\n",
    "This information will be used for running your Standard Risk Test against an existing model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {}\n",
    "response = session.get(moc_url + 'model-manage/api/storedModels/' + model_selector.value)\n",
    "if response.ok:\n",
    "\tmodel = response.json()\n",
    "\tdisplay(VBox([\n",
    "\t\tHBox([Label(value=\"Model Name:\"), Label(model.get(\"modelMetaData\", {}).get(\"name\", \"Name not set\"))]),\n",
    "\t\tHBox([Label(value=\"Description:\"), Label(model.get(\"modelMetaData\", \"\").get(\"description\", \"\"))]),\n",
    "\t\tHBox([Label(value=\"Created By:\"), Label(model.get(\"createdBy\", \"Unknown\"))]),\n",
    "\t\tHBox([Label(value=\"Created Date:\"), Label(model.get(\"createdDate\", \"Unknown\"))])\n",
    "\t]))\n",
    "\tdisplay(HTML(\"Model loaded and can be viewed <a href=\" + moc_url + \"#/models/business-models/\" + model[\"id\"] + \" target=\\\"_blank\\\" >here</a>.\"))\n",
    "else:\n",
    "\tprint(\"ERROR - The model could not be found, please update the example_model_name variable with the correct model name to examine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine example model assets\n",
    "\n",
    "Run the cells below, and we will attempt to locate the required model assets for you.  If specific\n",
    "assets are not automatically recognized we will let you know what assets are available that could\n",
    "potentially provide the needed data.  The asset role in ModelOp Center can then be changed to the appropriate type\n",
    "such as BASELINE_DATA or COMPARATOR_DATA.  There is also an option to use the training data as the baseline data for\n",
    "development purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5050f360bcfc4e8e817b4df97988b76c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Checkbox(value=False, description='Use training data as baseline data')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_training_data_checkbox = widgets.Checkbox(value=False, description='Use training data as baseline data')\n",
    "display(use_training_data_checkbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "baseline_data_file = None\n",
    "comparator_data_file = None\n",
    "unknown_data_files = []\n",
    "required_assets_df = None\n",
    "required_assets_validations = []\n",
    "\n",
    "out = widgets.Output()\n",
    "with out:\n",
    "\tif (os.path.exists('required_assets.json')):\n",
    "\t\tdisplay(HTML(\"</br>Here are the required assets for the standard risk test model and dashboard model: \"))\n",
    "\t\trequired_assets_df = pd.read_json('required_assets.json')\n",
    "\t\trequired_assets_validations = [widgets.Valid(value=False, description=f'{role}') for role in required_assets_df[\"assetRole\"]]\n",
    "\t\tdisplay(required_assets_df)\n",
    "\telse:\n",
    "\t\tdisplay(HTML(\"</br>File 'required_assets.json' not found on the standard risk test model. It is recommended that you include this file in your model definition with a list of required assets.\"))\n",
    "display(out)\n",
    "\n",
    "display(HTML(\"</br>Here are the current assets on the model:\"))\n",
    "model_assets_df = pd.DataFrame(model[\"modelAssets\"]).replace(np.nan, '')\n",
    "model_assets_df[\"asset name\"] = model_assets_df[['filename', 'name']].agg(' : '.join, axis=1)\n",
    "model_assets_df = model_assets_df[[\"asset name\",\"assetType\", \"assetRole\", \"fileFormat\"]]\n",
    "\n",
    "def highlight(v):\n",
    "\treturn f\"background-color:yellow;color:green;\" if required_assets_df is not None and v in required_assets_df[\"assetRole\"].to_list() else None\n",
    "s = model_assets_df.style.applymap(highlight) \n",
    "display(s)\n",
    "\n",
    "def setValid(required_assets_validations, asset_role, filename):\n",
    "\tfor i in required_assets_validations:\n",
    "\t\tif i.description == asset_role:\n",
    "\t\t\ti.value = True\n",
    "\t\t\ti.filename = filename\n",
    "\n",
    "supported_asset_types = ['EXTERNAL_FILE', 'FILE']\n",
    "for asset in model[\"modelAssets\"]:\n",
    "\tif supported_asset_types.count(asset[\"assetType\"]) != 0:\n",
    "\t\tif asset[\"assetRole\"] == 'BASELINE_DATA' or (use_training_data_checkbox.value == True and asset[\"assetRole\"] == 'TRAINING_DATA'):\n",
    "\t\t\tbaseline_data_file = asset\n",
    "\t\t\tsetValid(required_assets_validations, \"BASELINE_DATA\", asset[\"filename\"])\n",
    "\t\telif asset[\"assetRole\"] == 'COMPARATOR_DATA':\n",
    "\t\t\tcomparator_data_file = asset\n",
    "\t\t\tsetValid(required_assets_validations, \"COMPARATOR_DATA\", asset[\"filename\"])\n",
    "\t\telif (asset[\"filename\"].endswith(\".json\") or asset[\"filename\"].endswith(\".csv\")):\n",
    "\t\t\tunknown_data_files.append(asset)\n",
    "\n",
    "\n",
    "[display(HBox([validation, Label(f\"{validation.filename if validation.value else ''}\")])) for validation in required_assets_validations]\n",
    "if baseline_data_file is None:\n",
    "\tdisplay(HTML(\"Could not find the required baseline data file.  Please select an existing asset as the baseline data (list provided below) or upload the baseline data into the system in the window below\"))\n",
    "\tdisplay(use_training_data_checkbox)\n",
    "if comparator_data_file is None:\n",
    "\tdisplay(HTML(\"Could not find the required comparator data file.  Please select an existing asset as the comparator data (list provided below) or upload the comparator data into the system in the window below\"))\n",
    "if (baseline_data_file is None or comparator_data_file is None):\n",
    "\tdisplay(HTML(\"Other registered assets that could be the data you are looking for:\"))\n",
    "\tif len(unknown_data_files) == 0:\n",
    "\t\tdisplay(HTML(\"- No other candidate assets found.\"))\n",
    "\tfor asset in unknown_data_files:\n",
    "\t\tdisplay(HTML(\"- \" + asset[\"filename\"] + \" -> \" + asset[\"assetRole\"]))\n",
    "\n",
    "display(HTML(\"You can view and edit the assets <a target=\\\"_blank\\\" href=\\\"\" + moc_url + \"#/models/business-models/\" + model[\"id\"] + \"/assets\\\">here</a>.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the Baseline Data\n",
    "\n",
    "Now we will validate that we can successfully read in the baseline data from the s3 location the asset specifies, or\n",
    "from the embedded data if it is an embedded file type.  A small sample of the data will be displayed for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "import hashlib\n",
    "import hmac\n",
    "from minio import Minio\n",
    "import boto3\n",
    "\n",
    "baseline_df = None\n",
    "if (baseline_data_file[\"assetType\"] == 'EXTERNAL_FILE'):\n",
    "\ttry:\n",
    "\t\ts3_client = Minio(baseline_data_file[\"repositoryInfo\"][\"host\"] + \":\" + str(baseline_data_file[\"repositoryInfo\"][\"port\"]),\n",
    "\t\t\t\t\t\t  access_key=s3_access_key,\n",
    "\t\t\t\t\t\t  secret_key=s3_secret_key,\n",
    "\t\t\t\t\t\t  secure=baseline_data_file[\"repositoryInfo\"][\"secure\"])\n",
    "\t\tfile = s3_client.get_object(baseline_data_file[\"repositoryInfo\"][\"host\"],\n",
    "\t\t\t\t\t\t\t\t\tbaseline_data_file[\"name\"])\n",
    "\t\tbaseline_df = pd.read_json(file, lines=True)\n",
    "\texcept:\n",
    "\t\tkey = bytes(client_secret, 'utf-8')\n",
    "\t\tmessage = bytes(f'{s3_access_key}{client_id}', 'utf-8')\n",
    "\t\thash = base64.b64encode(hmac.new(key, message, digestmod=hashlib.sha256).digest()).decode()\n",
    "\t\tclient = boto3.client(\"cognito-idp\", region_name=\"us-east-1\")\n",
    "\t\tresponse = client.initiate_auth(\n",
    "    \t\tClientId=client_id,\n",
    "    \t\tAuthFlow=\"USER_PASSWORD_AUTH\",\n",
    "    \t\tAuthParameters={\"USERNAME\": s3_access_key, \"PASSWORD\": s3_secret_key, \"SECRET_HASH\": hash}\n",
    "\t\t)\n",
    "\t\tidentity = boto3.client('cognito-identity', region_name = 'us-east-1')\n",
    "\t\tidentity_token = response[\"AuthenticationResult\"][\"IdToken\"]\n",
    "\t\tidentityId = identity.get_id(\n",
    "       \t\tIdentityPoolId=cognito_identity_pool_id,\n",
    "        \tLogins={\n",
    "            \t\tcognito_user_pool_id : identity_token\n",
    "        \t\t}\n",
    "    \t)['IdentityId']\n",
    "\t\taws_cred = identity.get_credentials_for_identity(\n",
    "\t\t\tIdentityId=identityId,\n",
    "\t\t\tLogins={\n",
    "    \t\t\tcognito_user_pool_id: identity_token\n",
    "        \t}\n",
    "    \t)['Credentials']\n",
    "\t\tsts_client = boto3.client('sts',\n",
    "\t\t\taws_access_key_id=aws_cred['AccessKeyId'],\n",
    "    \t\taws_secret_access_key=aws_cred['SecretKey'],\n",
    "    \t\taws_session_token=aws_cred['SessionToken'],\n",
    "\t\t)\n",
    "\t\tassumed_role_object=sts_client.assume_role(\n",
    "\t\t\tRoleArn=role_arn,\n",
    "\t\t\tRoleSessionName=\"AssumeRoleSessionS3\"\n",
    "\t\t)\n",
    "\t\tcredentials=assumed_role_object['Credentials']\n",
    "\t\ts3=boto3.client(\n",
    "    \t\t's3',\n",
    "\t\t\tregion_name='us-east-2',\n",
    "    \t\taws_access_key_id=credentials['AccessKeyId'],\n",
    "    \t\taws_secret_access_key=credentials['SecretAccessKey'],\n",
    "    \t\taws_session_token=credentials['SessionToken'],\n",
    "\t\t)\n",
    "\t\tbucket=baseline_data_file[\"repositoryInfo\"][\"host\"].split('.')[0]\n",
    "\t\tfile = s3.get_object(Bucket=bucket, Key=baseline_data_file[\"name\"])[\"Body\"].read()\n",
    "\t\tbaseline_df = pd.read_json(file, lines=True)\n",
    "elif baseline_data_file[\"assetType\"] == 'FILE':\n",
    "\tbaseline_df = pd.read_json(baseline_data_file[\"fileContentString\"], lines=True)\n",
    "if baseline_df is not None:\n",
    "\tfor key in baseline_df.columns:\n",
    "\t\tif column_labels.count(key) == 0:\n",
    "\t\t\tprint(\"WARN - Found column '\" + str(key) + \"' in data but there is no matching schema entry in the input schema\")\n",
    "\tdisplay(HTML(\"Baseline data file:  \" + baseline_data_file[\"filename\"]))\n",
    "\tdisplay(baseline_df)\n",
    "else:\n",
    "\tdisplay(HTML(\"<h2>Could not read the provided baseline data asset<h2>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the Comparator Data\n",
    "\n",
    "Now we will validate that we can successfully read in the comparator data from the s3 location the asset specifies, or\n",
    "from the embedded data if it is an embedded file type.  A small sample of the data will be displayed for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "comparator_df = None\n",
    "if (comparator_data_file[\"assetType\"] == 'EXTERNAL_FILE'):\n",
    "\ttry:\n",
    "\t\ts3_client = Minio(comparator_data_file[\"repositoryInfo\"][\"host\"] + \":\" + str(comparator_data_file[\"repositoryInfo\"][\"port\"]),\n",
    "\t\t\t\t\t\t  access_key=s3_access_key,\n",
    "\t\t\t\t\t\t  secret_key=s3_secret_key,\n",
    "\t\t\t\t\t\t  secure=comparator_data_file[\"repositoryInfo\"][\"secure\"])\n",
    "\t\tfile = s3_client.get_object(comparator_data_file[\"repositoryInfo\"][\"host\"],\n",
    "\t\t\t\t\t\t\t\t\tcomparator_data_file[\"name\"])\n",
    "\t\tcomparator_df = pd.read_json(file, lines=True)\n",
    "\texcept:\n",
    "\t\tkey = bytes(client_secret, 'utf-8')\n",
    "\t\tmessage = bytes(f'{s3_access_key}{client_id}', 'utf-8')\n",
    "\t\thash = base64.b64encode(hmac.new(key, message, digestmod=hashlib.sha256).digest()).decode()\n",
    "\t\tclient = boto3.client(\"cognito-idp\", region_name=\"us-east-1\")\n",
    "\t\tresponse = client.initiate_auth(\n",
    "    \t\tClientId=client_id,\n",
    "    \t\tAuthFlow=\"USER_PASSWORD_AUTH\",\n",
    "    \t\tAuthParameters={\"USERNAME\": s3_access_key, \"PASSWORD\": s3_secret_key, \"SECRET_HASH\": hash}\n",
    "\t\t)\n",
    "\t\tidentity = boto3.client('cognito-identity', region_name = 'us-east-1')\n",
    "\t\tidentity_token = response[\"AuthenticationResult\"][\"IdToken\"]\n",
    "\t\tidentityId = identity.get_id(\n",
    "       \t\tIdentityPoolId=cognito_identity_pool_id,\n",
    "        \tLogins={\n",
    "            \t\tcognito_user_pool_id : identity_token\n",
    "        \t\t}\n",
    "    \t)['IdentityId']\n",
    "\t\taws_cred = identity.get_credentials_for_identity(\n",
    "\t\t\tIdentityId=identityId,\n",
    "\t\t\tLogins={\n",
    "    \t\t\tcognito_user_pool_id: identity_token\n",
    "        \t}\n",
    "    \t)['Credentials']\n",
    "\t\tsts_client = boto3.client('sts',\n",
    "\t\t\taws_access_key_id=aws_cred['AccessKeyId'],\n",
    "    \t\taws_secret_access_key=aws_cred['SecretKey'],\n",
    "    \t\taws_session_token=aws_cred['SessionToken'],\n",
    "\t\t)\n",
    "\t\tassumed_role_object=sts_client.assume_role(\n",
    "\t\t\tRoleArn=role_arn,\n",
    "\t\t\tRoleSessionName=\"AssumeRoleSessionS3\"\n",
    "\t\t)\n",
    "\t\tcredentials=assumed_role_object['Credentials']\n",
    "\t\ts3=boto3.client(\n",
    "    \t\t's3',\n",
    "\t\t\tregion_name='us-east-2',\n",
    "    \t\taws_access_key_id=credentials['AccessKeyId'],\n",
    "    \t\taws_secret_access_key=credentials['SecretAccessKey'],\n",
    "    \t\taws_session_token=credentials['SessionToken'],\n",
    "\t\t)\n",
    "\t\tbucket=baseline_data_file[\"repositoryInfo\"][\"host\"].split('.')[0]\n",
    "\t\tfile = s3.get_object(Bucket=bucket, Key=baseline_data_file[\"name\"])[\"Body\"].read()\n",
    "\t\tcomparator_df = pd.read_json(file, lines=True)\n",
    "elif comparator_data_file[\"assetType\"] == 'FILE':\n",
    "\tcomparator_df = pd.read_json(comparator_data_file[\"fileContentString\"], lines=True)\n",
    "if comparator_df is not None:\n",
    "\tfor key in comparator_df.columns:\n",
    "\t\tif column_labels.count(key) == 0:\n",
    "\t\t\tprint(\"WARN - Found column '\" + str(key) + \"' in data but there is no matching schema entry in the input schema\")\n",
    "\tdisplay(HTML(\"Comparator data file: \" + comparator_data_file[\"filename\"]))\n",
    "\tdisplay(comparator_df)\n",
    "else:\n",
    "\tdisplay(HTML(\"<h2>Could not read the provided comparator data asset</h2>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Standard Risk Tests Model\n",
    "\n",
    "Paste your model metrics and init function here.  Your init function should follow the format of:\n",
    "```\n",
    "# modelop.init\n",
    "def init(job_json):\n",
    "    global DEPLOYABLE_MODEL\n",
    "    global JOB\n",
    "    global MODEL_METHODOLOGY\n",
    "\n",
    "    job = json.loads(job_json[\"rawJson\"])\n",
    "    DEPLOYABLE_MODEL = job[\"referenceModel\"]\n",
    "    MODEL_METHODOLOGY = DEPLOYABLE_MODEL.get(\"storedModel\", {}).get(\"modelMetaData\", {}).get(\"modelMethodology\", \"\")\n",
    "\n",
    "    JOB = job_json\n",
    "    infer.validate_schema(job_json)\n",
    "```\n",
    "\n",
    "This sample provides the example of how you can read information from the job and model for use in your standardized\n",
    "test or dashboard.\n",
    "\n",
    "Additionally, the metrics function follows the format:\n",
    "```\n",
    "# modelop.metrics\n",
    "def metrics(baseline, comparator) -> dict:\n",
    "\tyield result\n",
    "```\n",
    "It will receive two panda dataframes with the data from the assets loaded above.  You can then use that data for any\n",
    "calculations you need, as well as feed those into the standard ModelOp monitors package.  Additionally, you can call out\n",
    "to other services or databases for additional dashboard style service health, etc.  You must yield the resulting\n",
    "dictionary at the end as the runtime engine utilizes the generator pattern to invoke your metrics function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import modelop.monitors.bias as bias\n",
    "import modelop.monitors.drift as drift\n",
    "import modelop.monitors.performance as performance\n",
    "import modelop.monitors.stability as stability\n",
    "import modelop.schema.infer as infer\n",
    "import modelop.stats.diagnostics as diagnostics\n",
    "import modelop.utils as utils\n",
    "from modelop_sdk.utils import dashboard_utils as dashboard_utils\n",
    "\n",
    "DEPLOYABLE_MODEL = {}\n",
    "JOB = {}\n",
    "MODEL_METHODOLOGY = \"\"\n",
    "\n",
    "\n",
    "# modelop.init\n",
    "def init(job_json):\n",
    "    global DEPLOYABLE_MODEL\n",
    "    global JOB\n",
    "    global MODEL_METHODOLOGY\n",
    "\n",
    "    job = json.loads(job_json[\"rawJson\"])\n",
    "    DEPLOYABLE_MODEL = job[\"referenceModel\"]\n",
    "    MODEL_METHODOLOGY = DEPLOYABLE_MODEL.get(\"storedModel\", {}).get(\"modelMetaData\", {}).get(\"modelMethodology\", \"\")\n",
    "\n",
    "    JOB = job_json\n",
    "    infer.validate_schema(job_json)\n",
    "\n",
    "\n",
    "# modelop.metrics\n",
    "def metrics(baseline, comparator) -> dict:\n",
    "\n",
    "    execution_errors_array = []\n",
    "\n",
    "    result = utils.merge(\n",
    "        extract_model_fields(execution_errors_array),\n",
    "        calculate_performance(comparator, execution_errors_array),\n",
    "        calculate_bias(comparator, execution_errors_array),\n",
    "        calculate_ks_drift(baseline, comparator, execution_errors_array),\n",
    "        calculate_ks_concept_drift(baseline, comparator, execution_errors_array),\n",
    "        calculate_stability(baseline, comparator, execution_errors_array),\n",
    "        calculate_breusch_pagan(comparator, execution_errors_array),\n",
    "        calculate_linearity_metrics(comparator, execution_errors_array),\n",
    "        calculate_ljung_box_q_test(comparator, execution_errors_array),\n",
    "        calculate_variance_inflation_factor(comparator, execution_errors_array),\n",
    "        calculate_durbin_watson(comparator, execution_errors_array),\n",
    "        calculate_engle_lagrange_multiplier_test(comparator, execution_errors_array),\n",
    "        calculate_anderson_darling_test(comparator, execution_errors_array),\n",
    "        calculate_cramer_von_mises_test(comparator, execution_errors_array),\n",
    "        calculate_kolmogorov_smirnov_test(comparator, execution_errors_array),\n",
    "    )\n",
    "\n",
    "    result.update({\"executionErrors\": execution_errors_array})\n",
    "    result.update({\"executionErrorsCount\": len(execution_errors_array)})\n",
    "\n",
    "    yield result\n",
    "\n",
    "\n",
    "def extract_model_fields(execution_errors_array):\n",
    "    try:\n",
    "        return {\n",
    "            \"modelUseCategory\": DEPLOYABLE_MODEL.get(\"storedModel\", {})\n",
    "                .get(\"modelMetaData\", {})\n",
    "                .get(\"modelUseCategory\", \"\"),\n",
    "            \"modelOrganization\": DEPLOYABLE_MODEL.get(\"storedModel\", {})\n",
    "                .get(\"modelMetaData\", {})\n",
    "                .get(\"modelOrganization\", \"\"),\n",
    "            \"modelRisk\": DEPLOYABLE_MODEL.get(\"storedModel\", {})\n",
    "                .get(\"modelMetaData\", {})\n",
    "                .get(\"modelRisk\", \"\"),\n",
    "            \"modelMethodology\": MODEL_METHODOLOGY\n",
    "        }\n",
    "    except Exception as ex:\n",
    "        error_message = f\"Something went wrong when extracting modelop default fields: {str(ex)}\"\n",
    "        execution_errors_array.append(error_message)\n",
    "        print(error_message)\n",
    "        return {}\n",
    "\n",
    "\n",
    "def calculate_performance(comparator, execution_errors_array):\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            comparator, \"Required comparator\"\n",
    "        )\n",
    "        model_evaluator = performance.ModelEvaluator(dataframe=comparator, job_json=JOB)\n",
    "        if \"regression\" in MODEL_METHODOLOGY.casefold():\n",
    "            return model_evaluator.evaluate_performance(\n",
    "                pre_defined_metrics=\"regression_metrics\"\n",
    "            )\n",
    "        else:\n",
    "            return model_evaluator.evaluate_performance(\n",
    "                pre_defined_metrics=\"classification_metrics\"\n",
    "            )\n",
    "    except Exception as ex:\n",
    "        error_message = f\"Error occurred calculating performance metrics: {str(ex)}\"\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"auc\": -99, \"r2_score\": 99}\n",
    "\n",
    "\n",
    "def calculate_bias(comparator, execution_errors_array):\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            comparator, \"Required comparator\"\n",
    "        )\n",
    "        bias_monitor = bias.BiasMonitor(dataframe=comparator, job_json=JOB)\n",
    "        if \"regression\" in MODEL_METHODOLOGY.casefold():\n",
    "            raise Exception(\"Bias metrics can not be run for regression models.\")\n",
    "        else:\n",
    "            return bias_monitor.compute_bias_metrics(pre_defined_test=\"aequitas_bias\")\n",
    "    except Exception as ex:\n",
    "        error_message = f\"Error occurred calculating bias metrics: {str(ex)}\"\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"Bias_maxPPRDisparityValue\": -99, \"Bias_minPPRDisparityValue\": -99}\n",
    "\n",
    "\n",
    "def calculate_ks_drift(baseline, sample, execution_errors_array):\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(baseline, \"Required baseline\")\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(sample, \"Required comparator\")\n",
    "        drift_test = drift.DriftDetector(\n",
    "            df_baseline=baseline, df_sample=sample, job_json=JOB\n",
    "        )\n",
    "        return drift_test.calculate_drift(pre_defined_test=\"Kolmogorov-Smirnov\")\n",
    "    except Exception as ex:\n",
    "        error_message = f\"Error occurred while calculating drift: {str(ex)}\"\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"DataDrift_maxKolmogorov-SmirnovPValue\": -99}\n",
    "\n",
    "\n",
    "def calculate_ks_concept_drift(baseline, sample, execution_errors_array):\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(baseline, \"Required baseline\")\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(sample, \"Required comparator\")\n",
    "        concept_drift_test = drift.ConceptDriftDetector(\n",
    "            df_baseline=baseline, df_sample=sample, job_json=JOB\n",
    "        )\n",
    "        return concept_drift_test.calculate_concept_drift(\n",
    "            pre_defined_test=\"Kolmogorov-Smirnov\"\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        error_message = f\"Error occurred while calculating concept drift: {str(ex)}\"\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"ConceptDrift_maxKolmogorov-SmirnovPValueValue\": -99}\n",
    "\n",
    "\n",
    "def calculate_stability(df_baseline, df_comparator, execution_errors_array):\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            df_baseline, \"Required baseline\"\n",
    "        )\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            df_comparator, \"Required comparator\"\n",
    "        )\n",
    "        stability_test = stability.StabilityMonitor(\n",
    "            df_baseline=df_baseline, df_sample=df_comparator, job_json=JOB\n",
    "        )\n",
    "        return stability_test.compute_stability_indices()\n",
    "    except Exception as ex:\n",
    "        error_message = f\"Error occurred while calculating stability: {str(ex)}\"\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"CSI_maxCSIValue\": -99}\n",
    "\n",
    "\n",
    "def calculate_breusch_pagan(dataframe, execution_errors_array):\n",
    "    \"\"\"A function to run the Breauch-Pagan test on sample data\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Sample prod data containing scores (model outputs),\n",
    "        labels (ground truths) and numerical_columns (predictors)\n",
    "        execution_errors_array (array): Array for collecting execution errors\n",
    "    Returns:\n",
    "        (dict): Breusch-Pagan test results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if \"regression\" in MODEL_METHODOLOGY.casefold():\n",
    "            dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "                dataframe, \"Required comparator\"\n",
    "            )\n",
    "            homoscedasticity_metrics = diagnostics.HomoscedasticityMetrics(\n",
    "                dataframe=dataframe, job_json=JOB\n",
    "            )\n",
    "            return homoscedasticity_metrics.breusch_pagan_test()\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Breusch-Pagan metrics can only be run for regression models.\"\n",
    "            )\n",
    "    except Exception as ex:\n",
    "        error_message = f\"Error occurred while calculating breusch_pagan: {str(ex)}\"\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"breusch_pagan_f_p_value\": -99}\n",
    "\n",
    "\n",
    "def calculate_variance_inflation_factor(dataframe, execution_errors_array):\n",
    "    \"\"\"A function to compute Variance Inflation Factors on sample data\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Sample prod data containing numerical_columns (predictors)\n",
    "        execution_errors_array (array): Array for collecting execution errors\n",
    "    Returns:\n",
    "        (dict): Pearson Correlation results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            dataframe, \"Required comparator\"\n",
    "        )\n",
    "        # dataframe=dataframe.astype('float')\n",
    "        multicollinearity_metrics = diagnostics.MulticollinearityMetrics(\n",
    "            dataframe=dataframe, job_json=JOB\n",
    "        )\n",
    "        return multicollinearity_metrics.variance_inflation_factor()\n",
    "    except Exception as ex:\n",
    "        error_message = (\n",
    "            f\"Error occurred while calculating variance_inflation_factor: {str(ex)}\"\n",
    "        )\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"Multicollinearity_maxVIFValue\": -99}\n",
    "\n",
    "\n",
    "def calculate_linearity_metrics(dataframe, execution_errors_array):\n",
    "    \"\"\"A function to compute Pearson Correlations on sample data\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Sample prod data containing scores (model outputs)\n",
    "        and numerical_columns (predictors)\n",
    "        execution_errors_array (array): Array for collecting execution errors\n",
    "    Returns:\n",
    "        (dict): Pearson Correlation results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            dataframe, \"Required comparator\"\n",
    "        )\n",
    "        linearity_metrics = diagnostics.LinearityMetrics(\n",
    "            dataframe=dataframe, job_json=JOB\n",
    "        )\n",
    "        return linearity_metrics.pearson_correlation()\n",
    "    except Exception as ex:\n",
    "        error_message = (\n",
    "            f\"Error occurred while calculating calculate_linearity_metrics: {str(ex)}\"\n",
    "        )\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"Linearity_minPearsonCorrelationValue\": -99}\n",
    "\n",
    "\n",
    "def calculate_ljung_box_q_test(dataframe, execution_errors_array):\n",
    "    \"\"\"A function to run the Ljung-Box Q test on sample data\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Sample prod data containing scores (model outputs),\n",
    "        labels (ground truths) and numerical_columns (predictors)\n",
    "        execution_errors_array (array): Array for collecting execution errors\n",
    "    Returns:\n",
    "        (dict): Ljung-Box Q test results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if \"regression\" in MODEL_METHODOLOGY.casefold():\n",
    "            dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "                dataframe, \"Required comparator\"\n",
    "            )\n",
    "            homoscedasticity_metrics = diagnostics.HomoscedasticityMetrics(\n",
    "                dataframe=dataframe, job_json=JOB\n",
    "            )\n",
    "            return homoscedasticity_metrics.ljung_box_q_test()\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Ljung-Box Q metrics can only be run for regression models.\"\n",
    "            )\n",
    "    except Exception as ex:\n",
    "        error_message = (\n",
    "            f\"Error occurred while calculating calculate_ljung_box_q_test: {str(ex)}\"\n",
    "        )\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"Homoscedasticity_minLjungBoxQPValue\": -99}\n",
    "\n",
    "\n",
    "def calculate_durbin_watson(dataframe, execution_errors_array):\n",
    "    \"\"\"A function to run the Durban Watson test on sample data\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Sample prod data containing scores (model outputs) and\n",
    "        labels (ground truths)\n",
    "        execution_errors_array (array): Array for collecting execution errors\n",
    "    Returns:\n",
    "        (dict): Durbin-Watson test results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            dataframe, \"Required comparator\"\n",
    "        )\n",
    "        autocorrelation_metrics = diagnostics.AutocorrelationMetrics(\n",
    "            dataframe=dataframe, job_json=JOB\n",
    "        )\n",
    "        return autocorrelation_metrics.durbin_watson_test()\n",
    "    except Exception as ex:\n",
    "        error_message = (\n",
    "            f\"Error occurred while calculating durban_watson test: {str(ex)}\"\n",
    "        )\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"dw_statistic\": -99}\n",
    "\n",
    "\n",
    "def calculate_engle_lagrange_multiplier_test(dataframe, execution_errors_array):\n",
    "    \"\"\"A function to run the engle_lagrange_multiplier_test on sample data\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Sample prod data containing scores (model outputs),\n",
    "        labels (ground truths) and numerical_columns (predictors)\n",
    "        execution_errors_array (array): Array for collecting execution errors\n",
    "    Returns:\n",
    "        (dict): Engle's Langrange Multiplier test results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if \"regression\" in MODEL_METHODOLOGY.casefold():\n",
    "            dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "                dataframe, \"Required comparator\"\n",
    "            )\n",
    "            homoscedasticity_metrics = diagnostics.HomoscedasticityMetrics(\n",
    "                dataframe=dataframe, job_json=JOB\n",
    "            )\n",
    "            return homoscedasticity_metrics.engle_lagrange_multiplier_test()\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Engle's Langrange Multiplier metrics can only be run for regression models.\"\n",
    "            )\n",
    "    except Exception as ex:\n",
    "        error_message = f\"Error occurred while calculating engle_lagrange_multiplier test: {str(ex)}\"\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"engle_lm_p_value\": -99}\n",
    "\n",
    "\n",
    "def calculate_anderson_darling_test(dataframe, execution_errors_array):\n",
    "    \"\"\"A function to run the calculate_anderson_darling_test on sample data\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Sample prod data containing scores (model outputs) and\n",
    "        labels (ground truths)\n",
    "        execution_errors_array (array): Array for collecting execution errors\n",
    "    Returns:\n",
    "        (dict): Anderson-Darling test results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            dataframe, \"Required comparator\"\n",
    "        )\n",
    "        normality_metrics = diagnostics.NormalityMetrics(\n",
    "            dataframe=dataframe, job_json=JOB\n",
    "        )\n",
    "        return normality_metrics.anderson_darling_test()\n",
    "    except Exception as ex:\n",
    "        error_message = (\n",
    "            f\"Error occurred while calculating anderson_darling test: {str(ex)}\"\n",
    "        )\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"ad_p_value\": -99}\n",
    "\n",
    "\n",
    "def calculate_cramer_von_mises_test(dataframe, execution_errors_array):\n",
    "    \"\"\"A function to run the cramer_von_mises_test on sample data\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Sample prod data containing scores (model outputs) and\n",
    "        labels (ground truths)\n",
    "        execution_errors_array (array): Array for collecting execution errors\n",
    "    Returns:\n",
    "        (dict): Cramer-von Mises test results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            dataframe, \"Required comparator\"\n",
    "        )\n",
    "        normality_metrics = diagnostics.NormalityMetrics(\n",
    "            dataframe=dataframe, job_json=JOB\n",
    "        )\n",
    "        return normality_metrics.cramer_von_mises_test()\n",
    "    except Exception as ex:\n",
    "        error_message = (\n",
    "            f\"Error occurred while calculating cramer_von_mises test: {str(ex)}\"\n",
    "        )\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"cvm_p_value\": -99}\n",
    "\n",
    "\n",
    "def calculate_kolmogorov_smirnov_test(dataframe, execution_errors_array):\n",
    "    \"\"\"A function to run the kolmogorov_smirnov_test on sample data\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Sample prod data containing scores (model outputs) and\n",
    "        labels (ground truths)\n",
    "        execution_errors_array (array): Array for collecting execution errors\n",
    "    Returns:\n",
    "        (dict): Kolmogorov-Smirnov test results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            dataframe, \"Required comparator\"\n",
    "        )\n",
    "        normality_metrics = diagnostics.NormalityMetrics(\n",
    "            dataframe=dataframe, job_json=JOB\n",
    "        )\n",
    "        return normality_metrics.kolmogorov_smirnov_test()\n",
    "    except Exception as ex:\n",
    "        error_message = (\n",
    "            f\"Error occurred while calculating kolmogorov_smirnov test: {str(ex)}\"\n",
    "        )\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"ks_p_value\": -99}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Example Job\n",
    "\n",
    "This will generate an example job that can be used to call your init param in the init function of your model, this does not need to be created by you and is automatically generated from your data. This is an internal data structure specific to ModelOp, which is used in our monitors and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job = {\"deployableModel\": {\"storedModel\": model},\n",
    "\t   \"inputData\" : [baseline_data_file, comparator_data_file],\n",
    "\t   \"referenceModel\": {\"storedModel\": model},\n",
    "\t   \"additionalAssets\": []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model Init Function\n",
    "\n",
    "Now let's run the model's init function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "init({'rawJson' : json.dumps(job)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model Metrics Function\n",
    "\n",
    "Now we can run the metrics function and capture the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metrics_output = widgets.Output(layout={'border': '1px solid black'})\n",
    "with metrics_output:\n",
    "\tresult = metrics(baseline_df, comparator_df)\n",
    "\tfirst_result = next(result)\n",
    "display(metrics_output)\n",
    "display(HTML(\"<div><pre><code>\" + json.dumps(first_result, indent=1, sort_keys=True) + \"</code></pre></div>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Run Complete\n",
    "\n",
    "See your results above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
