{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard Risk Test Developer\n",
    "\n",
    "## Provide ModelOp Center and S3 Configuration\n",
    "\n",
    "In order to read information from ModelOp Center, we must provide the location of where ModelOp Center is installed and\n",
    "our S3 access credentials.  Please run the cell and fill them below and click the Update button."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame, display, HTML\n",
    "import os\n",
    "import requests\n",
    "from ipywidgets import widgets, HBox, VBox, Label\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "moc_url = \"http://localhost:8090/\"\n",
    "#moc_url = \"http://mocaasin.modelop.center/\"\n",
    "os.environ[\"modelop.gateway-url\"] = moc_url\n",
    "config_out = widgets.Output(layout={'border': '1px solid black'})\n",
    "s3_access_key = \"AKIAIOSFODNN7EXAMPLE\"\n",
    "s3_secret_key = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n",
    "\n",
    "def update_values(button):\n",
    "\tglobal moc_url, s3_access_key, s3_secret_key\n",
    "\tmoc_url = moc_url_box.value\n",
    "\ts3_access_key = s3_access_key_box.value\n",
    "\ts3_secret_key = s3_secret_key_box.value\n",
    "\twith config_out:\n",
    "\t\tprint(\"Using modelop center url: \" + moc_url)\n",
    "\t\tprint(\"S3 Keys Updated\")\n",
    "\n",
    "display(widgets.Label('ModelOp Center URL'))\n",
    "moc_url_box = widgets.Text(value=moc_url)\n",
    "display(moc_url_box)\n",
    "display(widgets.Label('S3 Access Key'))\n",
    "s3_access_key_box = widgets.Password(value=s3_access_key)\n",
    "display(s3_access_key_box)\n",
    "display(widgets.Label('S3 Secret Key'))\n",
    "s3_secret_key_box = widgets.Password(value=s3_secret_key)\n",
    "display(s3_secret_key_box)\n",
    "update_button = widgets.Button(description='Update Values')\n",
    "update_button.on_click(update_values)\n",
    "display(update_button)\n",
    "display(config_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "\n",
    "require.undef('sign_in_widget');\n",
    "\n",
    "function getJupyterUrl () {\n",
    "    return location.protocol+'//' + location.hostname + (location.port ? ':'+location.port: '');\n",
    "}\n",
    "\n",
    "function removeEndingSlash( url ) {\n",
    "    return url.replace(/\\/$/, \"\");\n",
    "}\n",
    "\n",
    "function randomString(length) {\n",
    "    var chars = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ1234567890,./;'[]\\=-)(*&^%$#@!~`\";\n",
    "    var result = \"\";\n",
    "    for (var i = length; i > 0; --i) result += chars[Math.floor(Math.random() * chars.length)];\n",
    "    return result;\n",
    "}\n",
    "\n",
    "define('sign_in_widget', [\"@jupyter-widgets/base\"], function(widgets) {\n",
    "\n",
    "    var SignInView = widgets.DOMWidgetView.extend({\n",
    "\n",
    "        getOAuth2Token: function( widget, customPagePath, finalBaseUrl ) {\n",
    "            finalBaseUrl = removeEndingSlash(finalBaseUrl)\n",
    "            $.ajax({\n",
    "                type: 'GET',\n",
    "                url: finalBaseUrl + '/api/oauth2/.well-known-configuration/jupyter',\n",
    "                success: function( response ) {\n",
    "                    widget.redirectToLogin(widget, customPagePath, finalBaseUrl, response);\n",
    "                },\n",
    "                error: function( jqXHR, textStatus, error ) {\n",
    "                    if (jqXHR.status == 404) {\n",
    "                        widget.login_result.textContent = \"Login not required (or no well known configuration found for jupyter)\";\n",
    "                        widget.model.save_changes();\n",
    "                    } else {\n",
    "                        console.log(\"An error occurred while trying to get the authorization uri from gateway service.\");\n",
    "                        widget.login_result.textContent = \"Couldn't connect with the provided Url: \" + finalBaseUrl;\n",
    "                    }\n",
    "                }\n",
    "            });\n",
    "        },\n",
    "\n",
    "        redirectToLogin: function (widget, customPagePath, finalBaseUrl, oAuth2Data ) {\n",
    "            let formattedScope = oAuth2Data.scopes.join(\" \");\n",
    "\n",
    "            var redirectUriParams = {\n",
    "                client_id: oAuth2Data.clientId,\n",
    "                response_type: oAuth2Data.responseType,\n",
    "                scope: formattedScope,\n",
    "                state: getJupyterUrl() + customPagePath,\n",
    "                nonce: randomString(10),\n",
    "                redirect_uri: oAuth2Data.redirectUri\n",
    "            };\n",
    "            widget.displayClientAuthorization(widget, oAuth2Data.oAuth2Provider.authorizationUri + \"?\" + $.param(redirectUriParams));\n",
    "        },\n",
    "\n",
    "        displayClientAuthorization: function (widget, url ) {\n",
    "            var w = 500;\n",
    "            var h = 600;\n",
    "            const y = window.top.outerHeight / 2 + window.top.screenY - ( h / 2);\n",
    "            const x = window.top.outerWidth / 2 + window.top.screenX - ( w / 2);\n",
    "            var login_popup = window.open(url, 'Client Authorization', `toolbar=no, location=no, directories=no, status=no, menubar=no, scrollbars=no, resizable=no, copyhistory=no, width=${w}, height=${h}, top=${y}, left=${x}`);\n",
    "            var timer = setInterval(function() {\n",
    "                try {\n",
    "                    if(login_popup.closed) {\n",
    "                        clearInterval(timer);\n",
    "                        if (!widget.token_input.value) {\n",
    "                            widget.login_result.textContent = \"Login interrupted!\"\n",
    "                        }\n",
    "                    }\n",
    "                    try {\n",
    "                        if (login_popup.location?.hash) {\n",
    "                            let parsedHash = new URLSearchParams(login_popup.location.hash.substring(1));\n",
    "                            widget.model.set('token', parsedHash.get(\"access_token\"));\n",
    "                            widget.login_result.textContent = \"Login successful!\"\n",
    "                            widget.model.save_changes();\n",
    "                            login_popup.close()\n",
    "                        }\n",
    "                    } catch {}\n",
    "                } catch {\n",
    "                    clearInterval(timer);\n",
    "                }\n",
    "            }, 200);\n",
    "        },\n",
    "    \n",
    "        // Render the view.\n",
    "        render: function() {\n",
    "            this.getOAuth2Token(this, \"/\", this.model.get('base_url'))\n",
    "            this.login_result = document.createElement('div');\n",
    "            this.token_input = document.createElement('hidden');\n",
    "            this.token_input.type = 'text';\n",
    "            this.token_input.value = this.model.get('token');\n",
    "            this.token_input.disabled = this.model.get('disabled');\n",
    "\n",
    "            this.el.appendChild(this.login_result);\n",
    "            this.el.appendChild(this.token_input);\n",
    "\n",
    "            // Python -> JavaScript update\n",
    "            this.model.on('change:token', this.value_changed, this);\n",
    "            this.model.on('change:disabled', this.disabled_changed, this);\n",
    "\n",
    "            // JavaScript -> Python update\n",
    "            this.token_input.onchange = this.input_changed.bind(this);\n",
    "        },\n",
    "\n",
    "        value_changed: function() {\n",
    "            this.token_input.value = this.model.get('token');\n",
    "        },\n",
    "\n",
    "        disabled_changed: function() {\n",
    "            this.token_input.disabled = this.model.get('disabled');\n",
    "        },\n",
    "\n",
    "        input_changed: function() {\n",
    "            this.model.set('token', this.token_input.value);\n",
    "            this.model.save_changes();\n",
    "        },\n",
    "    });\n",
    "\n",
    "    return {\n",
    "        SignInView: SignInView\n",
    "    };\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from traitlets import Unicode, Bool\n",
    "from ipywidgets import DOMWidget, register\n",
    "\n",
    "\n",
    "@register\n",
    "class SignInWidget(DOMWidget):\n",
    "    _view_name = Unicode('SignInView').tag(sync=True)\n",
    "    _view_module = Unicode('sign_in_widget').tag(sync=True)\n",
    "    _view_module_version = Unicode('0.1.0').tag(sync=True)\n",
    "\n",
    "    # Attributes\n",
    "    token = Unicode(\"\", help=\"The oauth2 token obtained.\").tag(sync=True)\n",
    "    disabled = Bool(True, help=\"Enable or disable user changes.\").tag(sync=True)\n",
    "    base_url = Unicode(\"http://localhost:8090\", help=\"The base URL\").tag(sync=True)\n",
    "\n",
    "\n",
    "login = SignInWidget(base_url = moc_url)\n",
    "display(login)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choose Target Model\n",
    "\n",
    "A dashboard or standardized test model runs against a given target model.  It will pull the required data resources,\n",
    "such as baseline and/or comparator data, from this target model.  So in developing your dashboard or standardized test,\n",
    "we will start with picking an example target model that is currently registered with ModelOp Center to run our dashboard\n",
    "or test against.  This model should already have baseline and comparator data defined as assets stored in S3.  Run the\n",
    "cell and select one of the models in the drop down box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "session = requests.Session()\n",
    "session.headers.update({'Authorization': f'Bearer {login.token}'})\n",
    "try:\n",
    "\tresponse = session.get(moc_url + 'model-manage/api/storedModelSummaries?size=500')\n",
    "\n",
    "\tavailable_models = []\n",
    "\tresponse.raise_for_status()\n",
    "\tif response.ok:\n",
    "\t\tmodel_list = response.json()\n",
    "\t\tfor model in model_list[\"_embedded\"][\"storedModelSummaries\"]:\n",
    "\t\t\tavailable_models.append((model[\"modelMetaData\"][\"name\"], model[\"id\"]))\n",
    "\tmodel_selector = widgets.Dropdown(options=available_models, description='Target Model:')\n",
    "\tdisplay(model_selector)\n",
    "except Exception as e:\n",
    "\tdisplay(HTML('Couldn\\'t connect to ' + moc_url + '</br><pre><code>' + str(e) + '</code></pre>'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Target Model\n",
    "\n",
    "By running the cell below, we will contact model manage and fetch the definition of the example target model.\n",
    "This information will be used for running your Standard Risk Test against an existing model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = {}\n",
    "response = session.get(moc_url + 'model-manage/api/storedModels/' + model_selector.value)\n",
    "if response.ok:\n",
    "\tmodel = response.json()\n",
    "\tdisplay(VBox([\n",
    "\t\tHBox([Label(value=\"Model Name:\"), Label(model.get(\"modelMetaData\", {}).get(\"name\", \"Name not set\"))]),\n",
    "\t\tHBox([Label(value=\"Description:\"), Label(model.get(\"modelMetaData\", \"\").get(\"description\", \"\"))]),\n",
    "\t\tHBox([Label(value=\"Created By:\"), Label(model.get(\"createdBy\", \"Unknown\"))]),\n",
    "\t\tHBox([Label(value=\"Created Date:\"), Label(model.get(\"createdDate\", \"Unknown\"))])\n",
    "\t]))\n",
    "\tdisplay(HTML(\"Model loaded and can be viewed <a href=\" + moc_url + \"#/models/model/\" + model[\"id\"] + \" target=\\\"_blank\\\" >here</a>.\"))\n",
    "else:\n",
    "\tprint(\"ERROR - The model could not be found, please update the example_model_name variable with the correct model name to examine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Check for valid schema\n",
    "\n",
    "Let's now check that the target model has the schema populated.  All target models for dashboards and standardized tests\n",
    "will typically require an input schema definition.  This allows us to know what columns are used for labels, inputs, and\n",
    "which columns are a protected class, as well as other pertinent information about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "column_labels = []\n",
    "input_schema = model.get(\"modelMetaData\", {}).get(\"inputSchema\", [])\n",
    "if len(input_schema) != 1:\n",
    "\tdisplay(HTML(\"The standard risk test model and dashboard model requires exactly one input schema to be specified. Please go to the model's schema page in MOC, and generate and edit a valid schema for the input data\"))\n",
    "else:\n",
    "\tinput_schema = model.get(\"modelMetaData\", {}).get(\"inputSchema\", [])\n",
    "\tresponse = session.post(moc_url + \"model-manage/api/schema/validate/?extended=true\", json = input_schema[0].get(\"schemaDefinition\", {}))\n",
    "\tif response.ok:\n",
    "\t\tdisplay(HTML(\"The example model contains a valid schema\"))\n",
    "\t\tfor col in input_schema[0].get(\"schemaDefinition\", {}).get(\"fields\", []):\n",
    "\t\t\tcolumn_labels.append(col.get(\"name\", \"\"))\n",
    "\t\tdisplay(column_labels)\n",
    "\telse:\n",
    "\t\tdisplay(HTML(\"The standard risk test model and dashboard model requires a valid extended schema definition. Please go to the model's schema page in MOC, and generate and edit a valid schema for the input data\"))\n",
    "display(HTML(\"You can view and edit the schema <a target=\\\"_blank\\\" href=\\\"\" + moc_url + \"#/models/model/\" + model[\"id\"] + \"/schema\\\">here</a>.\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine example model assets\n",
    "\n",
    "Run the cells below, and we will attempt to locate the required model assets for you.  If specific\n",
    "assets are not automatically recognized we will let you know what assets are available that could\n",
    "potentially provide the needed data.  The asset role in ModelOp Center can then be changed to the appropriate type\n",
    "such as BASELINE_DATA or COMPARATOR_DATA.  There is also an option to use the training data as the baseline data for\n",
    "development purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_training_data_checkbox = widgets.Checkbox(value=False, description='Use training data as baseline data')\n",
    "display(use_training_data_checkbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "baseline_data_file = None\n",
    "comparator_data_file = None\n",
    "unknown_data_files = []\n",
    "required_assets_df = None\n",
    "required_assets_validations = []\n",
    "\n",
    "out = widgets.Output()\n",
    "with out:\n",
    "\tif (os.path.exists('required_assets.json')):\n",
    "\t\tdisplay(HTML(\"</br>Here are the required assets for the standard risk test model and dashboard model: \"))\n",
    "\t\trequired_assets_df = pd.read_json('required_assets.json')\n",
    "\t\trequired_assets_validations = [widgets.Valid(value=False, description=f'{role}') for role in required_assets_df[\"assetRole\"]]\n",
    "\t\tdisplay(required_assets_df)\n",
    "\telse:\n",
    "\t\tdisplay(HTML(\"</br>File 'required_assets.json' not found on the standard risk test model. It is recommended that you include this file in your model definition with a list of required assets.\"))\n",
    "display(out)\n",
    "\n",
    "display(HTML(\"</br>Here are the current assets on the model:\"))\n",
    "model_assets_df = pd.DataFrame(model[\"modelAssets\"]).replace(np.nan, '')\n",
    "model_assets_df[\"asset name\"] = model_assets_df[['filename', 'name']].agg(' : '.join, axis=1)\n",
    "model_assets_df = model_assets_df[[\"asset name\",\"assetType\", \"assetRole\", \"fileFormat\"]]\n",
    "\n",
    "def highlight(v):\n",
    "\treturn f\"background-color:yellow;color:green;\" if required_assets_df is not None and v in required_assets_df[\"assetRole\"].to_list() else None\n",
    "s = model_assets_df.style.applymap(highlight) \n",
    "display(s)\n",
    "\n",
    "def setValid(required_assets_validations, asset_role, filename):\n",
    "\tfor i in required_assets_validations:\n",
    "\t\tif i.description == asset_role:\n",
    "\t\t\ti.value = True\n",
    "\t\t\ti.filename = filename\n",
    "\n",
    "supported_asset_types = ['EXTERNAL_FILE', 'FILE']\n",
    "for asset in model[\"modelAssets\"]:\n",
    "\tif supported_asset_types.count(asset[\"assetType\"]) != 0:\n",
    "\t\tif asset[\"assetRole\"] == 'BASELINE_DATA' or (use_training_data_checkbox.value == True and asset[\"assetRole\"] == 'TRAINING_DATA'):\n",
    "\t\t\tbaseline_data_file = asset\n",
    "\t\t\tsetValid(required_assets_validations, \"BASELINE_DATA\", asset[\"filename\"])\n",
    "\t\telif asset[\"assetRole\"] == 'COMPARATOR_DATA':\n",
    "\t\t\tcomparator_data_file = asset\n",
    "\t\t\tsetValid(required_assets_validations, \"COMPARATOR_DATA\", asset[\"filename\"])\n",
    "\t\telif (asset[\"filename\"].endswith(\".json\") or asset[\"filename\"].endswith(\".csv\")):\n",
    "\t\t\tunknown_data_files.append(asset)\n",
    "\n",
    "\n",
    "[display(HBox([validation, Label(f\"{validation.filename if validation.value else ''}\")])) for validation in required_assets_validations]\n",
    "if baseline_data_file is None:\n",
    "\tdisplay(HTML(\"Could not find the required baseline data file.  Please select an existing asset as the baseline data (list provided below) or upload the baseline data into the system in the window below\"))\n",
    "\tdisplay(use_training_data_checkbox)\n",
    "if comparator_data_file is None:\n",
    "\tdisplay(HTML(\"Could not find the required comparator data file.  Please select an existing asset as the comparator data (list provided below) or upload the comparator data into the system in the window below\"))\n",
    "if (baseline_data_file is None or comparator_data_file is None):\n",
    "\tdisplay(HTML(\"Other registered assets that could be the data you are looking for:\"))\n",
    "\tif len(unknown_data_files) == 0:\n",
    "\t\tdisplay(HTML(\"- No other candidate assets found.\"))\n",
    "\tfor asset in unknown_data_files:\n",
    "\t\tdisplay(HTML(\"- \" + asset[\"filename\"] + \" -> \" + asset[\"assetRole\"]))\n",
    "\n",
    "display(HTML(\"You can view and edit the assets <a target=\\\"_blank\\\" href=\\\"\" + moc_url + \"#/models/model/\" + model[\"id\"] + \"/assets\\\">here</a>.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the Baseline Data\n",
    "\n",
    "Now we will validate that we can successfully read in the baseline data from the s3 location the asset specifies, or\n",
    "from the embedded data if it is an embedded file type.  A small sample of the data will be displayed for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from minio import Minio\n",
    "import boto3\n",
    "\n",
    "baseline_df = None\n",
    "if (baseline_data_file[\"assetType\"] == 'EXTERNAL_FILE'):\n",
    "\ttry:\n",
    "\t\ts3_client = Minio(baseline_data_file[\"repositoryInfo\"][\"host\"] + \":\" + str(baseline_data_file[\"repositoryInfo\"][\"port\"]),\n",
    "\t\t\t\t\t\t  access_key=s3_access_key,\n",
    "\t\t\t\t\t\t  secret_key=s3_secret_key,\n",
    "\t\t\t\t\t\t  secure=baseline_data_file[\"repositoryInfo\"][\"secure\"])\n",
    "\t\tfile = s3_client.get_object(baseline_data_file[\"repositoryInfo\"][\"host\"],\n",
    "\t\t\t\t\t\t\t\t\tbaseline_data_file[\"name\"])\n",
    "\t\tbaseline_df = pd.read_json(file, lines=True)\n",
    "\texcept:\n",
    "\t\ts3 = boto3.client('s3', region_name='us-east-2',\n",
    "\t\t\t\t\t\t\t\taws_access_key_id=s3_access_key,\n",
    "\t\t\t\t\t\t\t\taws_secret_access_key=s3_secret_key)\n",
    "\t\tbucket=baseline_data_file[\"repositoryInfo\"][\"host\"].split('.')[0]\n",
    "\t\tfile = s3.get_object(Bucket=bucket, Key=baseline_data_file[\"name\"])[\"Body\"].read()\n",
    "\t\tbaseline_df = pd.read_json(file, lines=True)\n",
    "elif baseline_data_file[\"assetType\"] == 'FILE':\n",
    "\tbaseline_df = pd.read_json(baseline_data_file[\"fileContentString\"], lines=True)\n",
    "if baseline_df is not None:\n",
    "\tfor key in baseline_df.columns:\n",
    "\t\tif column_labels.count(key) == 0:\n",
    "\t\t\tprint(\"WARN - Found column '\" + str(key) + \"' in data but there is no matching schema entry in the input schema\")\n",
    "\tdisplay(HTML(\"Baseline data file:  \" + baseline_data_file[\"filename\"]))\n",
    "\tdisplay(baseline_df)\n",
    "else:\n",
    "\tdisplay(HTML(\"<h2>Could not read the provided baseline data asset<h2>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the Comparator Data\n",
    "\n",
    "Now we will validate that we can successfully read in the comparator data from the s3 location the asset specifies, or\n",
    "from the embedded data if it is an embedded file type.  A small sample of the data will be displayed for verification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "comparator_df = None\n",
    "if (comparator_data_file[\"assetType\"] == 'EXTERNAL_FILE'):\n",
    "\ttry:\n",
    "\t\ts3_client = Minio(comparator_data_file[\"repositoryInfo\"][\"host\"] + \":\" + str(comparator_data_file[\"repositoryInfo\"][\"port\"]),\n",
    "\t\t\t\t\t\t  access_key=s3_access_key,\n",
    "\t\t\t\t\t\t  secret_key=s3_secret_key,\n",
    "\t\t\t\t\t\t  secure=comparator_data_file[\"repositoryInfo\"][\"secure\"])\n",
    "\t\tfile = s3_client.get_object(comparator_data_file[\"repositoryInfo\"][\"host\"],\n",
    "\t\t\t\t\t\t\t\t\tcomparator_data_file[\"name\"])\n",
    "\t\tcomparator_df = pd.read_json(file, lines=True)\n",
    "\texcept:\n",
    "\t\ts3 = boto3.client('s3', region_name='us-east-2',\n",
    "\t\t\t\t\t\t\t\taws_access_key_id=s3_access_key,\n",
    "\t\t\t\t\t\t\t\taws_secret_access_key=s3_secret_key)\n",
    "\t\tbucket=baseline_data_file[\"repositoryInfo\"][\"host\"].split('.')[0]\n",
    "\t\tfile = s3.get_object(Bucket=bucket, Key=baseline_data_file[\"name\"])[\"Body\"].read()\n",
    "\t\tcomparator_df = pd.read_json(file, lines=True)\n",
    "elif comparator_data_file[\"assetType\"] == 'FILE':\n",
    "\tcomparator_df = pd.read_json(comparator_data_file[\"fileContentString\"], lines=True)\n",
    "if comparator_df is not None:\n",
    "\tfor key in comparator_df.columns:\n",
    "\t\tif column_labels.count(key) == 0:\n",
    "\t\t\tprint(\"WARN - Found column '\" + str(key) + \"' in data but there is no matching schema entry in the input schema\")\n",
    "\tdisplay(HTML(\"Comparator data file: \" + comparator_data_file[\"filename\"]))\n",
    "\tdisplay(comparator_df)\n",
    "else:\n",
    "\tdisplay(HTML(\"<h2>Could not read the provided comparator data asset</h2>\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Standard Risk Tests Model\n",
    "\n",
    "Paste your model metrics and init function here.  Your init function should follow the format of:\n",
    "```\n",
    "# modelop.init\n",
    "def init(job_json):\n",
    "    global DEPLOYABLE_MODEL\n",
    "    global JOB\n",
    "    global MODEL_METHODOLOGY\n",
    "\n",
    "    job = json.loads(job_json[\"rawJson\"])\n",
    "    DEPLOYABLE_MODEL = job[\"referenceModel\"]\n",
    "    MODEL_METHODOLOGY = DEPLOYABLE_MODEL.get(\"storedModel\", {}).get(\"modelMetaData\", {}).get(\"modelMethodology\", \"\")\n",
    "\n",
    "    JOB = job_json\n",
    "    infer.validate_schema(job_json)\n",
    "```\n",
    "\n",
    "This sample provides the example of how you can read information from the job and model for use in your standardized\n",
    "test or dashboard.\n",
    "\n",
    "Additionally, the metrics function follows the format:\n",
    "```\n",
    "# modelop.metrics\n",
    "def metrics(baseline, comparator) -> dict:\n",
    "\tyield result\n",
    "```\n",
    "It will receive two panda dataframes with the data from the assets loaded above.  You can then use that data for any\n",
    "calculations you need, as well as feed those into the standard ModelOp monitors package.  Additionally, you can call out\n",
    "to other services or databases for additional dashboard style service health, etc.  You must yield the resulting\n",
    "dictionary at the end as the runtime engine utilizes the generator pattern to invoke your metrics function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import modelop.monitors.bias as bias\n",
    "import modelop.monitors.drift as drift\n",
    "import modelop.monitors.performance as performance\n",
    "import modelop.monitors.stability as stability\n",
    "import modelop.schema.infer as infer\n",
    "import modelop.stats.diagnostics as diagnostics\n",
    "import modelop.utils as utils\n",
    "from modelop_sdk.utils import dashboard_utils as dashboard_utils\n",
    "\n",
    "DEPLOYABLE_MODEL = {}\n",
    "JOB = {}\n",
    "MODEL_METHODOLOGY = \"\"\n",
    "\n",
    "\n",
    "# modelop.init\n",
    "def init(job_json):\n",
    "    global DEPLOYABLE_MODEL\n",
    "    global JOB\n",
    "    global MODEL_METHODOLOGY\n",
    "\n",
    "    job = json.loads(job_json[\"rawJson\"])\n",
    "    DEPLOYABLE_MODEL = job[\"referenceModel\"]\n",
    "    MODEL_METHODOLOGY = DEPLOYABLE_MODEL.get(\"storedModel\", {}).get(\"modelMetaData\", {}).get(\"modelMethodology\", \"\")\n",
    "\n",
    "    JOB = job_json\n",
    "    infer.validate_schema(job_json)\n",
    "\n",
    "\n",
    "# modelop.metrics\n",
    "def metrics(baseline, comparator) -> dict:\n",
    "\n",
    "    execution_errors_array = []\n",
    "\n",
    "    result = utils.merge(\n",
    "        extract_model_fields(execution_errors_array),\n",
    "        calculate_performance(comparator, execution_errors_array),\n",
    "        calculate_bias(comparator, execution_errors_array),\n",
    "        calculate_ks_drift(baseline, comparator, execution_errors_array),\n",
    "        calculate_ks_concept_drift(baseline, comparator, execution_errors_array),\n",
    "        calculate_stability(baseline, comparator, execution_errors_array),\n",
    "        calculate_breusch_pagan(comparator, execution_errors_array),\n",
    "        calculate_linearity_metrics(comparator, execution_errors_array),\n",
    "        calculate_ljung_box_q_test(comparator, execution_errors_array),\n",
    "        calculate_variance_inflation_factor(comparator, execution_errors_array),\n",
    "        calculate_durbin_watson(comparator, execution_errors_array),\n",
    "        calculate_engle_lagrange_multiplier_test(comparator, execution_errors_array),\n",
    "        calculate_anderson_darling_test(comparator, execution_errors_array),\n",
    "        calculate_cramer_von_mises_test(comparator, execution_errors_array),\n",
    "        calculate_kolmogorov_smirnov_test(comparator, execution_errors_array),\n",
    "    )\n",
    "\n",
    "    result.update({\"executionErrors\": execution_errors_array})\n",
    "    result.update({\"executionErrorsCount\": len(execution_errors_array)})\n",
    "\n",
    "    yield result\n",
    "\n",
    "\n",
    "def extract_model_fields(execution_errors_array):\n",
    "    try:\n",
    "        return {\n",
    "            \"modelUseCategory\": DEPLOYABLE_MODEL.get(\"storedModel\", {})\n",
    "                .get(\"modelMetaData\", {})\n",
    "                .get(\"modelUseCategory\", \"\"),\n",
    "            \"modelOrganization\": DEPLOYABLE_MODEL.get(\"storedModel\", {})\n",
    "                .get(\"modelMetaData\", {})\n",
    "                .get(\"modelOrganization\", \"\"),\n",
    "            \"modelRisk\": DEPLOYABLE_MODEL.get(\"storedModel\", {})\n",
    "                .get(\"modelMetaData\", {})\n",
    "                .get(\"modelRisk\", \"\"),\n",
    "            \"modelMethodology\": MODEL_METHODOLOGY\n",
    "        }\n",
    "    except Exception as ex:\n",
    "        error_message = f\"Something went wrong when extracting modelop default fields: {str(ex)}\"\n",
    "        execution_errors_array.append(error_message)\n",
    "        print(error_message)\n",
    "        return {}\n",
    "\n",
    "\n",
    "def calculate_performance(comparator, execution_errors_array):\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            comparator, \"Required comparator\"\n",
    "        )\n",
    "        model_evaluator = performance.ModelEvaluator(dataframe=comparator, job_json=JOB)\n",
    "        if \"regression\" in MODEL_METHODOLOGY.casefold():\n",
    "            return model_evaluator.evaluate_performance(\n",
    "                pre_defined_metrics=\"regression_metrics\"\n",
    "            )\n",
    "        else:\n",
    "            return model_evaluator.evaluate_performance(\n",
    "                pre_defined_metrics=\"classification_metrics\"\n",
    "            )\n",
    "    except Exception as ex:\n",
    "        error_message = f\"Error occurred calculating performance metrics: {str(ex)}\"\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"auc\": -99, \"r2_score\": 99}\n",
    "\n",
    "\n",
    "def calculate_bias(comparator, execution_errors_array):\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            comparator, \"Required comparator\"\n",
    "        )\n",
    "        bias_monitor = bias.BiasMonitor(dataframe=comparator, job_json=JOB)\n",
    "        if \"regression\" in MODEL_METHODOLOGY.casefold():\n",
    "            raise Exception(\"Bias metrics can not be run for regression models.\")\n",
    "        else:\n",
    "            return bias_monitor.compute_bias_metrics(pre_defined_test=\"aequitas_bias\")\n",
    "    except Exception as ex:\n",
    "        error_message = f\"Error occurred calculating bias metrics: {str(ex)}\"\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"Bias_maxPPRDisparityValue\": -99, \"Bias_minPPRDisparityValue\": -99}\n",
    "\n",
    "\n",
    "def calculate_ks_drift(baseline, sample, execution_errors_array):\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(baseline, \"Required baseline\")\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(sample, \"Required comparator\")\n",
    "        drift_test = drift.DriftDetector(\n",
    "            df_baseline=baseline, df_sample=sample, job_json=JOB\n",
    "        )\n",
    "        return drift_test.calculate_drift(pre_defined_test=\"Kolmogorov-Smirnov\")\n",
    "    except Exception as ex:\n",
    "        error_message = f\"Error occurred while calculating drift: {str(ex)}\"\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"DataDrift_maxKolmogorov-SmirnovPValue\": -99}\n",
    "\n",
    "\n",
    "def calculate_ks_concept_drift(baseline, sample, execution_errors_array):\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(baseline, \"Required baseline\")\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(sample, \"Required comparator\")\n",
    "        concept_drift_test = drift.ConceptDriftDetector(\n",
    "            df_baseline=baseline, df_sample=sample, job_json=JOB\n",
    "        )\n",
    "        return concept_drift_test.calculate_concept_drift(\n",
    "            pre_defined_test=\"Kolmogorov-Smirnov\"\n",
    "        )\n",
    "    except Exception as ex:\n",
    "        error_message = f\"Error occurred while calculating concept drift: {str(ex)}\"\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"ConceptDrift_maxKolmogorov-SmirnovPValueValue\": -99}\n",
    "\n",
    "\n",
    "def calculate_stability(df_baseline, df_comparator, execution_errors_array):\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            df_baseline, \"Required baseline\"\n",
    "        )\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            df_comparator, \"Required comparator\"\n",
    "        )\n",
    "        stability_test = stability.StabilityMonitor(\n",
    "            df_baseline=df_baseline, df_sample=df_comparator, job_json=JOB\n",
    "        )\n",
    "        return stability_test.compute_stability_indices()\n",
    "    except Exception as ex:\n",
    "        error_message = f\"Error occurred while calculating stability: {str(ex)}\"\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"CSI_maxCSIValue\": -99}\n",
    "\n",
    "\n",
    "def calculate_breusch_pagan(dataframe, execution_errors_array):\n",
    "    \"\"\"A function to run the Breauch-Pagan test on sample data\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Sample prod data containing scores (model outputs),\n",
    "        labels (ground truths) and numerical_columns (predictors)\n",
    "        execution_errors_array (array): Array for collecting execution errors\n",
    "    Returns:\n",
    "        (dict): Breusch-Pagan test results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if \"regression\" in MODEL_METHODOLOGY.casefold():\n",
    "            dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "                dataframe, \"Required comparator\"\n",
    "            )\n",
    "            homoscedasticity_metrics = diagnostics.HomoscedasticityMetrics(\n",
    "                dataframe=dataframe, job_json=JOB\n",
    "            )\n",
    "            return homoscedasticity_metrics.breusch_pagan_test()\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Breusch-Pagan metrics can only be run for regression models.\"\n",
    "            )\n",
    "    except Exception as ex:\n",
    "        error_message = f\"Error occurred while calculating breusch_pagan: {str(ex)}\"\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"breusch_pagan_f_p_value\": -99}\n",
    "\n",
    "\n",
    "def calculate_variance_inflation_factor(dataframe, execution_errors_array):\n",
    "    \"\"\"A function to compute Variance Inflation Factors on sample data\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Sample prod data containing numerical_columns (predictors)\n",
    "        execution_errors_array (array): Array for collecting execution errors\n",
    "    Returns:\n",
    "        (dict): Pearson Correlation results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            dataframe, \"Required comparator\"\n",
    "        )\n",
    "        # dataframe=dataframe.astype('float')\n",
    "        multicollinearity_metrics = diagnostics.MulticollinearityMetrics(\n",
    "            dataframe=dataframe, job_json=JOB\n",
    "        )\n",
    "        return multicollinearity_metrics.variance_inflation_factor()\n",
    "    except Exception as ex:\n",
    "        error_message = (\n",
    "            f\"Error occurred while calculating variance_inflation_factor: {str(ex)}\"\n",
    "        )\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"Multicollinearity_maxVIFValue\": -99}\n",
    "\n",
    "\n",
    "def calculate_linearity_metrics(dataframe, execution_errors_array):\n",
    "    \"\"\"A function to compute Pearson Correlations on sample data\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Sample prod data containing scores (model outputs)\n",
    "        and numerical_columns (predictors)\n",
    "        execution_errors_array (array): Array for collecting execution errors\n",
    "    Returns:\n",
    "        (dict): Pearson Correlation results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            dataframe, \"Required comparator\"\n",
    "        )\n",
    "        linearity_metrics = diagnostics.LinearityMetrics(\n",
    "            dataframe=dataframe, job_json=JOB\n",
    "        )\n",
    "        return linearity_metrics.pearson_correlation()\n",
    "    except Exception as ex:\n",
    "        error_message = (\n",
    "            f\"Error occurred while calculating calculate_linearity_metrics: {str(ex)}\"\n",
    "        )\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"Linearity_minPearsonCorrelationValue\": -99}\n",
    "\n",
    "\n",
    "def calculate_ljung_box_q_test(dataframe, execution_errors_array):\n",
    "    \"\"\"A function to run the Ljung-Box Q test on sample data\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Sample prod data containing scores (model outputs),\n",
    "        labels (ground truths) and numerical_columns (predictors)\n",
    "        execution_errors_array (array): Array for collecting execution errors\n",
    "    Returns:\n",
    "        (dict): Ljung-Box Q test results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if \"regression\" in MODEL_METHODOLOGY.casefold():\n",
    "            dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "                dataframe, \"Required comparator\"\n",
    "            )\n",
    "            homoscedasticity_metrics = diagnostics.HomoscedasticityMetrics(\n",
    "                dataframe=dataframe, job_json=JOB\n",
    "            )\n",
    "            return homoscedasticity_metrics.ljung_box_q_test()\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Ljung-Box Q metrics can only be run for regression models.\"\n",
    "            )\n",
    "    except Exception as ex:\n",
    "        error_message = (\n",
    "            f\"Error occurred while calculating calculate_ljung_box_q_test: {str(ex)}\"\n",
    "        )\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"Homoscedasticity_minLjungBoxQPValue\": -99}\n",
    "\n",
    "\n",
    "def calculate_durbin_watson(dataframe, execution_errors_array):\n",
    "    \"\"\"A function to run the Durban Watson test on sample data\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Sample prod data containing scores (model outputs) and\n",
    "        labels (ground truths)\n",
    "        execution_errors_array (array): Array for collecting execution errors\n",
    "    Returns:\n",
    "        (dict): Durbin-Watson test results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            dataframe, \"Required comparator\"\n",
    "        )\n",
    "        autocorrelation_metrics = diagnostics.AutocorrelationMetrics(\n",
    "            dataframe=dataframe, job_json=JOB\n",
    "        )\n",
    "        return autocorrelation_metrics.durbin_watson_test()\n",
    "    except Exception as ex:\n",
    "        error_message = (\n",
    "            f\"Error occurred while calculating durban_watson test: {str(ex)}\"\n",
    "        )\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"dw_statistic\": -99}\n",
    "\n",
    "\n",
    "def calculate_engle_lagrange_multiplier_test(dataframe, execution_errors_array):\n",
    "    \"\"\"A function to run the engle_lagrange_multiplier_test on sample data\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Sample prod data containing scores (model outputs),\n",
    "        labels (ground truths) and numerical_columns (predictors)\n",
    "        execution_errors_array (array): Array for collecting execution errors\n",
    "    Returns:\n",
    "        (dict): Engle's Langrange Multiplier test results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if \"regression\" in MODEL_METHODOLOGY.casefold():\n",
    "            dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "                dataframe, \"Required comparator\"\n",
    "            )\n",
    "            homoscedasticity_metrics = diagnostics.HomoscedasticityMetrics(\n",
    "                dataframe=dataframe, job_json=JOB\n",
    "            )\n",
    "            return homoscedasticity_metrics.engle_lagrange_multiplier_test()\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Engle's Langrange Multiplier metrics can only be run for regression models.\"\n",
    "            )\n",
    "    except Exception as ex:\n",
    "        error_message = f\"Error occurred while calculating engle_lagrange_multiplier test: {str(ex)}\"\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"engle_lm_p_value\": -99}\n",
    "\n",
    "\n",
    "def calculate_anderson_darling_test(dataframe, execution_errors_array):\n",
    "    \"\"\"A function to run the calculate_anderson_darling_test on sample data\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Sample prod data containing scores (model outputs) and\n",
    "        labels (ground truths)\n",
    "        execution_errors_array (array): Array for collecting execution errors\n",
    "    Returns:\n",
    "        (dict): Anderson-Darling test results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            dataframe, \"Required comparator\"\n",
    "        )\n",
    "        normality_metrics = diagnostics.NormalityMetrics(\n",
    "            dataframe=dataframe, job_json=JOB\n",
    "        )\n",
    "        return normality_metrics.anderson_darling_test()\n",
    "    except Exception as ex:\n",
    "        error_message = (\n",
    "            f\"Error occurred while calculating anderson_darling test: {str(ex)}\"\n",
    "        )\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"ad_p_value\": -99}\n",
    "\n",
    "\n",
    "def calculate_cramer_von_mises_test(dataframe, execution_errors_array):\n",
    "    \"\"\"A function to run the cramer_von_mises_test on sample data\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Sample prod data containing scores (model outputs) and\n",
    "        labels (ground truths)\n",
    "        execution_errors_array (array): Array for collecting execution errors\n",
    "    Returns:\n",
    "        (dict): Cramer-von Mises test results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            dataframe, \"Required comparator\"\n",
    "        )\n",
    "        normality_metrics = diagnostics.NormalityMetrics(\n",
    "            dataframe=dataframe, job_json=JOB\n",
    "        )\n",
    "        return normality_metrics.cramer_von_mises_test()\n",
    "    except Exception as ex:\n",
    "        error_message = (\n",
    "            f\"Error occurred while calculating cramer_von_mises test: {str(ex)}\"\n",
    "        )\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"cvm_p_value\": -99}\n",
    "\n",
    "\n",
    "def calculate_kolmogorov_smirnov_test(dataframe, execution_errors_array):\n",
    "    \"\"\"A function to run the kolmogorov_smirnov_test on sample data\n",
    "\n",
    "    Args:\n",
    "        dataframe (pandas.DataFrame): Sample prod data containing scores (model outputs) and\n",
    "        labels (ground truths)\n",
    "        execution_errors_array (array): Array for collecting execution errors\n",
    "    Returns:\n",
    "        (dict): Kolmogorov-Smirnov test results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dashboard_utils.assert_df_not_none_and_not_empty(\n",
    "            dataframe, \"Required comparator\"\n",
    "        )\n",
    "        normality_metrics = diagnostics.NormalityMetrics(\n",
    "            dataframe=dataframe, job_json=JOB\n",
    "        )\n",
    "        return normality_metrics.kolmogorov_smirnov_test()\n",
    "    except Exception as ex:\n",
    "        error_message = (\n",
    "            f\"Error occurred while calculating kolmogorov_smirnov test: {str(ex)}\"\n",
    "        )\n",
    "        print(error_message)\n",
    "        execution_errors_array.append(error_message)\n",
    "        return {\"ks_p_value\": -99}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Example Job\n",
    "\n",
    "This will generate an example job that can be used to call your init param in the init function of your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "job = {\"deployableModel\": {\"storedModel\": model},\n",
    "\t   \"inputData\" : [baseline_data_file, comparator_data_file],\n",
    "\t   \"referenceModel\": {\"storedModel\": model},\n",
    "\t   \"additionalAssets\": []}\n",
    "display(HTML(\"Example job: <pre><code>\" + json.dumps(job, indent=3, sort_keys=True) + \"</code></pre>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model Init Function\n",
    "\n",
    "Now let's run the model's init function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "init({'rawJson' : json.dumps(job)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model Metrics Function\n",
    "\n",
    "Now we can run the metrics function and capture the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"MODELOP_GATEWAY_LOCATION\"]=moc_url\n",
    "os.environ[\"MODELOP_OAUTH_ACCESS_TOKEN\"]=login.token\n",
    "\n",
    "metrics_output = widgets.Output(layout={'border': '1px solid black'})\n",
    "with metrics_output:\n",
    "\tresult = metrics(baseline_df, comparator_df)\n",
    "\tfirst_result = next(result)\n",
    "display(metrics_output)\n",
    "display(HTML(\"<div><pre><code>\" + json.dumps(first_result, indent=3, sort_keys=True) + \"</code></pre></div>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Run Complete\n",
    "\n",
    "See your results above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
